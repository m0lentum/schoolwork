{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIES483 Assignment, Mikael MyyrÃ¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A window is being built and the bottom is a rectangle and the top is a semicircle. If there is 12 m of framing materials what must the dimensions of the window be to make the window area as big as possible?\n",
    "\n",
    "Model the decision problem as an optimization problem and solve it with a method of your choosing. **Analyse the result!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant measurements (circumference and area) of the window can be expressed as functions of width $w$ and height $h$. The circumference is $w + 2h$ for the rectangular part and $\\pi \\frac{w}{2}$ for the circular part, so the total circumference is $C(w, h) = (1 + \\frac{\\pi}{2})w + 2h$. The area is $wh$ for the rectangular part and $\\pi (\\frac{w}{2})^2$ for the circular part, so the total area is $A(w, h) = wh + \\frac{\\pi}{4} w^2$. We're trying to maximize $A$ (and thus minimize $-A$) subject to the linear constraint $C = 12$. Also, $w$ and $h$ must be greater than zero because this is a real window. In formal terms, the problem is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\qquad & -wh - \\frac{\\pi}{4} w^2 \\\\\n",
    "\\text{s.t.} \\qquad & (1 + \\frac{\\pi}{2})w + 2h = 12 \\\\\n",
    "& w, r > 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple case it should be possible to find a solution analytically using the stationarity rule from the KKT conditions. Let's try it. The Lagrangian for this problem is\n",
    "\n",
    "$$\n",
    "L(w, h) = -wh - \\frac{\\pi}{4} w^2 - \\lambda(1 + \\frac{\\pi}{2})w - 2\\lambda h\n",
    "$$\n",
    "\n",
    "and its gradient\n",
    "\n",
    "$$\n",
    "\\nabla L(w, h) = (-h - \\frac{\\pi}{2} w - \\lambda(1 + \\frac{\\pi}{2}), -w - 2\\lambda).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking for points where\n",
    "$$\n",
    "\\nabla L(w, h) = \\mathbf{0} \\\\\n",
    "\\begin{cases}\n",
    "-h - \\frac{\\pi}{2}w - \\lambda(1 + \\frac{\\pi}{2}) = 0 \\\\\n",
    "-w - 2\\lambda = 0\n",
    "\\end{cases} \\\\\n",
    "w = -\\frac{\\lambda}{2} \\\\\n",
    "-h + (\\frac{\\pi}{4} - (1 + \\frac{\\pi}{2}))\\lambda = 0 \\iff h = (-\\frac{\\pi}{4} - 1)\\lambda\n",
    "$$\n",
    "\n",
    "Applying the constraint, we find feasible values for $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(1 + \\frac{\\pi}{2})w + 2h &= 12 \\\\\n",
    "(1 + \\frac{\\pi}{2})(-\\frac{\\lambda}{2}) + 2(-\\frac{\\pi}{4} - 1)\\lambda &= 12 \\\\x\n",
    "(-\\frac12 - \\frac{\\pi}{4} - \\frac{\\pi}{2} - 2)\\lambda &= 12 \\\\\n",
    "(-\\frac{3\\pi}{4} - \\frac{5}{2})\\lambda &= 12 \\\\\n",
    "\\lambda &= \\frac{-12}{\\frac{3\\pi}{4} + \\frac{5}{2}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "now, applying $\\lambda$ back to $w$ and $h$, we get\n",
    "\n",
    "$$\n",
    "w = -\\frac{\\lambda}{2} = \\frac{12}{\\frac{3\\pi}{2} + 5} \\approx 1.236 \\\\\n",
    "h = (-\\frac{\\pi}{4} - 1) \\lambda \n",
    "  = \\frac{3\\pi + 12}{\\frac{3\\pi}{4} + \\frac{5}{2}} \\approx 4.412.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: Because we used the KKT conditions to come up with this solution analytically, we already know it's optimal. Just to check that the calculations were correct, you can plug $w$ and $h$ back into the constraint function to ensure it holds. Plugging said values into the objective function and flipping the sign gives the optimal area of the window, which comes out to be approximately $6.653 m^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10-dimensional Robsenbrock function (one of the variants) is defined as\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_{i=1}^{9} 100 (x_{i+1} - x_i^2 )^2 + (1-x_i)^2\n",
    "$$\n",
    "for $x\\in\\mathbb R^{10}$. \n",
    "\n",
    "Compare at least two different optimization method's performance in minimizing this function over $\\mathbb R^{10}$. You can decide the method of comparison as the one that makes most sense to you. **Analyze the results!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's define the problem in Python\n",
    "\n",
    "def obj(x):\n",
    "    assert(len(x) == 10)\n",
    "    ans = 0.0\n",
    "    for i in range(0, 9):\n",
    "        ans += 100*((x[i+1] - x[i]**2)**2) + (1 - x[i])**2\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the function is also needed for a method I want to test. It's a bit complex so I'll write down some steps factorizing the function to make it easier for myself.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\mathbf{x}) &= \\sum_{i=1}^9 100(x_{i+1} - x_i^2)^2 + (1 - x_i)^2 \\\\\n",
    "&= \\sum_{i=1}^9 100(x_{i+1}^2 - 2 x_{i+1} x_i^2 + x_i^4) + (1 - 2 x_i + x_i^2) \\\\\n",
    "&= \\sum_{i=1}^9 100x_{i+1}^2 - 200 x_{i+1} x_i^2 + 100 x_i^4 + x_i^2 - 2 x_i + 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The expression for the gradient is so long it's hard to write down, but is fairly straightforward to calculate by iterating the sum and updating corresponding elements in the gradient with the derivatives of the summed expression. The gradient elements obtained from a single iteration of the sum are\n",
    "\n",
    "$$\n",
    "\\nabla_{(i, i+1)} f(\\mathbf{x}) = (-400x_{i+1}x_i + 400x_i^3 + 2x_i - 2, 200x_{i+1} - 200x_{i+1}x_i^2).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def obj_gradient(x):\n",
    "    assert(len(x) == 10)\n",
    "    ans = np.array([0.0] * 10)\n",
    "    for i in range(0, 9):\n",
    "        ans[i] += -400 * x[i+1] * x[i] + 400 * x[i]**3 + 2 * x[i] - 2\n",
    "        ans[i+1] += 200 * x[i+1] - 200 * x[i+1] * x[i]**2\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since scipy.optimize.minimize has multiple methods available and is able to print statistics, it seems like a convenient place to go for this task. Let's pick Nelder-Mead (uses no derivatives) and BFGS (uses first derivatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Starting point: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Nelder-Mead:\n",
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "Result:\n",
      " final_simplex: (array([[ 0.95819942,  0.92047379,  0.83391187,  0.70387608,  0.52461411,\n",
      "         0.28254586,  0.06659239,  0.08429688, -0.04589073, -0.08591718],\n",
      "       [ 0.95894233,  0.9204341 ,  0.83222897,  0.70545843,  0.5254473 ,\n",
      "         0.2806476 ,  0.06734214,  0.08417792, -0.04925657, -0.08365011],\n",
      "       [ 0.96120856,  0.92602729,  0.84155191,  0.72115311,  0.54131841,\n",
      "         0.29620842,  0.07283483,  0.08501551, -0.05263569, -0.08522277],\n",
      "       [ 0.95987139,  0.92438481,  0.83994796,  0.7175543 ,  0.54184927,\n",
      "         0.3029527 ,  0.07201315,  0.08518736, -0.04873201, -0.08613543],\n",
      "       [ 0.95692642,  0.92085713,  0.83469867,  0.70989731,  0.53229555,\n",
      "         0.28569522,  0.06754217,  0.08580009, -0.04813109, -0.08382746],\n",
      "       [ 0.95642002,  0.91996378,  0.83332828,  0.70289444,  0.51999602,\n",
      "         0.27972976,  0.06984152,  0.08314439, -0.04933047, -0.08598427],\n",
      "       [ 0.95946257,  0.92098857,  0.83384559,  0.70745784,  0.5240412 ,\n",
      "         0.27677058,  0.06734289,  0.08525029, -0.049609  , -0.08399479],\n",
      "       [ 0.95606883,  0.9173665 ,  0.82834773,  0.70270276,  0.52372669,\n",
      "         0.28204846,  0.06847622,  0.08306563, -0.04933393, -0.08358393],\n",
      "       [ 0.9600221 ,  0.92249905,  0.83678707,  0.71316977,  0.53120054,\n",
      "         0.2876331 ,  0.0698738 ,  0.08643524, -0.04921087, -0.08502075],\n",
      "       [ 0.95721847,  0.91773836,  0.82754518,  0.70007521,  0.52133314,\n",
      "         0.27975043,  0.06794376,  0.08456359, -0.04839973, -0.0821389 ],\n",
      "       [ 0.95646391,  0.91850892,  0.8306873 ,  0.70388411,  0.52501665,\n",
      "         0.28522892,  0.06952631,  0.08515477, -0.04838668, -0.08352602]]), array([5.49475967, 5.49903842, 5.49957241, 5.50037358, 5.50073397,\n",
      "       5.50097299, 5.50135384, 5.50273935, 5.50294226, 5.50308465,\n",
      "       5.50318537]))\n",
      "           fun: 5.494759667492491\n",
      "       message: 'Maximum number of function evaluations has been exceeded.'\n",
      "          nfev: 2000\n",
      "           nit: 1458\n",
      "        status: 1\n",
      "       success: False\n",
      "             x: array([ 0.95819942,  0.92047379,  0.83391187,  0.70387608,  0.52461411,\n",
      "        0.28254586,  0.06659239,  0.08429688, -0.04589073, -0.08591718])\n",
      "\n",
      "BFGS:\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 8.620272\n",
      "         Iterations: 7\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 76\n",
      "Result:\n",
      "      fun: 8.620271958944095\n",
      " hess_inv: array([[ 4.68343923e-02, -6.00084517e-02, -1.95639249e-02,\n",
      "        -1.92416200e-02, -2.85789007e-02,  1.11106242e-01,\n",
      "         3.12604776e-04,  8.30543901e-03,  1.55089155e-03,\n",
      "         0.00000000e+00],\n",
      "       [-6.00084517e-02,  8.10590926e-01, -1.76409335e-01,\n",
      "        -1.76697617e-01, -1.75786720e-01, -2.01305431e-01,\n",
      "         8.39784909e-02, -8.89978003e-02, -4.28000632e-02,\n",
      "         0.00000000e+00],\n",
      "       [-1.95639249e-02, -1.76409335e-01,  8.05204051e-01,\n",
      "        -1.95104899e-01, -1.94125274e-01, -2.16470888e-01,\n",
      "        -3.83412433e-02,  1.30474232e-02,  1.61546092e-02,\n",
      "         0.00000000e+00],\n",
      "       [-1.92416200e-02, -1.76697617e-01, -1.95104899e-01,\n",
      "         8.04584365e-01, -1.94430726e-01, -2.16910379e-01,\n",
      "        -3.76266091e-02,  1.38722821e-02,  1.61291670e-02,\n",
      "         0.00000000e+00],\n",
      "       [-2.85789007e-02, -1.75786720e-01, -1.94125274e-01,\n",
      "        -1.94430726e-01,  8.06516289e-01, -2.15147052e-01,\n",
      "        -3.93842162e-02,  1.23057235e-02,  1.79326411e-02,\n",
      "         0.00000000e+00],\n",
      "       [ 1.11106242e-01, -2.01305431e-01, -2.16470888e-01,\n",
      "        -2.16910379e-01, -2.15147052e-01,  7.44716549e-01,\n",
      "         3.16270450e-02,  3.87219544e-02, -8.12758136e-03,\n",
      "         0.00000000e+00],\n",
      "       [ 3.12604776e-04,  8.39784909e-02, -3.83412433e-02,\n",
      "        -3.76266091e-02, -3.93842162e-02,  3.16270450e-02,\n",
      "         1.78127478e-02, -7.76498304e-03, -6.27453110e-03,\n",
      "         0.00000000e+00],\n",
      "       [ 8.30543901e-03, -8.89978003e-02,  1.30474232e-02,\n",
      "         1.38722821e-02,  1.23057235e-02,  3.87219544e-02,\n",
      "        -7.76498304e-03,  1.54743282e-02,  4.09107947e-03,\n",
      "         0.00000000e+00],\n",
      "       [ 1.55089155e-03, -4.28000632e-02,  1.61546092e-02,\n",
      "         1.61291670e-02,  1.79326411e-02, -8.12758136e-03,\n",
      "        -6.27453110e-03,  4.09107947e-03,  8.22306506e-03,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         1.00000000e+00]])\n",
      "      jac: array([-0.14001436,  0.30954489,  0.25355373,  0.25417123,  0.26000047,\n",
      "        0.19927732, -0.29928511, -0.34773033,  0.50339937,  0.        ])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 88\n",
      "      nit: 7\n",
      "     njev: 76\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([0.18105005, 0.01209581, 0.01141292, 0.01141635, 0.01143744,\n",
      "       0.011074  , 0.00856132, 0.00838471, 0.01239016, 0.        ])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Starting point: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Nelder-Mead:\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 386\n",
      "         Function evaluations: 600\n",
      "Result:\n",
      " final_simplex: (array([[1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
      "       [1.00000001, 0.99999999, 0.99999999, 0.99999999, 0.99999999,\n",
      "        1.        , 0.99999999, 1.00000001, 1.00000003, 1.00000007],\n",
      "       [1.        , 1.        , 1.00000001, 0.99999999, 0.99999998,\n",
      "        0.99999996, 0.99999992, 0.99999987, 0.99999974, 0.99999947],\n",
      "       [1.        , 1.        , 1.00000001, 1.00000001, 1.        ,\n",
      "        1.00000001, 1.        , 0.99999999, 0.99999994, 0.99999988],\n",
      "       [1.        , 1.        , 1.00000002, 1.        , 0.99999998,\n",
      "        0.99999999, 0.99999996, 0.99999994, 0.9999999 , 0.99999981],\n",
      "       [1.00000002, 0.99999999, 1.        , 1.        , 1.        ,\n",
      "        0.99999998, 0.99999997, 0.99999996, 0.9999999 , 0.99999984],\n",
      "       [1.        , 0.99999999, 1.00000002, 1.00000001, 1.00000002,\n",
      "        1.00000002, 1.00000003, 1.0000001 , 1.00000016, 1.00000032],\n",
      "       [0.99999999, 1.00000001, 1.00000001, 1.00000001, 1.00000001,\n",
      "        1.        , 0.99999999, 1.00000003, 1.00000004, 1.0000001 ],\n",
      "       [1.00000001, 1.00000001, 1.00000001, 0.99999998, 0.99999996,\n",
      "        0.99999996, 0.99999994, 0.99999991, 0.99999983, 0.99999967],\n",
      "       [1.        , 1.00000001, 1.00000003, 1.        , 1.00000002,\n",
      "        1.00000002, 1.00000001, 1.00000004, 1.00000008, 1.0000002 ],\n",
      "       [0.99999997, 0.99999998, 0.99999999, 0.99999998, 0.99999999,\n",
      "        0.99999998, 0.99999996, 0.99999992, 0.99999984, 0.99999972]]), array([0.00000000e+00, 2.48122931e-13, 2.63639554e-13, 2.99659915e-13,\n",
      "       3.14808643e-13, 4.70344137e-13, 5.24313850e-13, 5.69781819e-13,\n",
      "       5.82396162e-13, 6.60679608e-13, 6.81552884e-13]))\n",
      "           fun: 0.0\n",
      "       message: 'Optimization terminated successfully.'\n",
      "          nfev: 600\n",
      "           nit: 386\n",
      "        status: 0\n",
      "       success: True\n",
      "             x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "\n",
      "BFGS:\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Result:\n",
      "      fun: 0.0\n",
      " hess_inv: array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
      "      jac: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 1\n",
      "      nit: 0\n",
      "     njev: 1\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Starting point: [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "Nelder-Mead:\n",
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "Result:\n",
      " final_simplex: (array([[0.99594434, 1.00061315, 0.99899839, 0.99894412, 1.00421332,\n",
      "        1.01327232, 1.01922871, 1.03531195, 1.0651717 , 1.13458539],\n",
      "       [0.99579141, 1.00105939, 0.99869556, 0.998967  , 1.0043477 ,\n",
      "        1.01188324, 1.01592046, 1.02636383, 1.0466271 , 1.09758113],\n",
      "       [0.99605162, 1.00083998, 0.99939916, 0.99842367, 1.00409885,\n",
      "        1.0119609 , 1.01652826, 1.02638101, 1.04683471, 1.09866375],\n",
      "       [0.99567335, 1.00064177, 0.99960383, 1.00014888, 1.00429823,\n",
      "        1.01392193, 1.01794475, 1.03268623, 1.05995148, 1.12322089],\n",
      "       [0.99696348, 1.00287916, 0.99978023, 0.99895393, 1.00450958,\n",
      "        1.01203808, 1.01501725, 1.02643147, 1.04707779, 1.09823168],\n",
      "       [0.99626718, 1.00122233, 0.99862005, 0.99973699, 1.0030887 ,\n",
      "        1.0136106 , 1.02077926, 1.03623261, 1.0670106 , 1.1409193 ],\n",
      "       [0.99588546, 1.00120275, 0.99969038, 0.99877669, 1.00350816,\n",
      "        1.01356892, 1.02016055, 1.03525761, 1.06553596, 1.13659802],\n",
      "       [0.99688491, 1.0009279 , 0.99870239, 0.99868266, 1.00506105,\n",
      "        1.0145002 , 1.01931836, 1.03570257, 1.06586327, 1.136172  ],\n",
      "       [0.99737973, 1.00171612, 0.99956403, 0.99800886, 1.00425776,\n",
      "        1.01309708, 1.01793983, 1.0311909 , 1.05572826, 1.11701668],\n",
      "       [0.99534629, 0.99860967, 0.99948626, 0.99944796, 1.0048926 ,\n",
      "        1.01558462, 1.02305132, 1.04202053, 1.079289  , 1.16549611],\n",
      "       [0.99643822, 1.00128601, 0.99894513, 0.99980725, 1.0050296 ,\n",
      "        1.01453359, 1.02090193, 1.03910217, 1.07462104, 1.16126319]]), array([0.03191272, 0.03353316, 0.03382018, 0.03425723, 0.03468514,\n",
      "       0.0357417 , 0.03576506, 0.03584489, 0.03595241, 0.03598218,\n",
      "       0.03621141]))\n",
      "           fun: 0.03191272070758676\n",
      "       message: 'Maximum number of function evaluations has been exceeded.'\n",
      "          nfev: 2001\n",
      "           nit: 1433\n",
      "        status: 1\n",
      "       success: False\n",
      "             x: array([0.99594434, 1.00061315, 0.99899839, 0.99894412, 1.00421332,\n",
      "       1.01327232, 1.01922871, 1.03531195, 1.0651717 , 1.13458539])\n",
      "\n",
      "BFGS:\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 1650.521452\n",
      "         Iterations: 1\n",
      "         Function evaluations: 42\n",
      "         Gradient evaluations: 30\n",
      "Result:\n",
      "      fun: 1650.5214524211724\n",
      " hess_inv: array([[ 0.22403795,  0.24325457, -0.05962776, -0.05962776, -0.05962776,\n",
      "        -0.05962776, -0.05962776, -0.05962776, -0.31876285, -0.04017804],\n",
      "       [ 0.24325457,  1.1709444 ,  0.09494023,  0.09494023,  0.09494023,\n",
      "         0.09494023,  0.09494023,  0.09494023,  0.02991382, -0.33815077],\n",
      "       [-0.05962776,  0.09494023,  1.01893605,  0.01893605,  0.01893605,\n",
      "         0.01893605,  0.01893605,  0.01893605, -0.04609036, -0.11127263],\n",
      "       [-0.05962776,  0.09494023,  0.01893605,  1.01893605,  0.01893605,\n",
      "         0.01893605,  0.01893605,  0.01893605, -0.04609036, -0.11127263],\n",
      "       [-0.05962776,  0.09494023,  0.01893605,  0.01893605,  1.01893605,\n",
      "         0.01893605,  0.01893605,  0.01893605, -0.04609036, -0.11127263],\n",
      "       [-0.05962776,  0.09494023,  0.01893605,  0.01893605,  0.01893605,\n",
      "         1.01893605,  0.01893605,  0.01893605, -0.04609036, -0.11127263],\n",
      "       [-0.05962776,  0.09494023,  0.01893605,  0.01893605,  0.01893605,\n",
      "         0.01893605,  1.01893605,  0.01893605, -0.04609036, -0.11127263],\n",
      "       [-0.05962776,  0.09494023,  0.01893605,  0.01893605,  0.01893605,\n",
      "         0.01893605,  0.01893605,  1.01893605, -0.04609036, -0.11127263],\n",
      "       [-0.31876285,  0.02991382, -0.04609036, -0.04609036, -0.04609036,\n",
      "        -0.04609036, -0.04609036, -0.04609036,  0.88888323,  0.08283606],\n",
      "       [-0.04017804, -0.33815077, -0.11127263, -0.11127263, -0.11127263,\n",
      "        -0.11127263, -0.11127263, -0.11127263,  0.08283606,  1.49558132]])\n",
      "      jac: array([  -72.60864294,   848.3269479 ,   249.06029234,   249.06029234,\n",
      "         249.06029234,   249.06029234,   249.06029234,   249.06029234,\n",
      "        -263.6504525 , -1175.12562657])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 42\n",
      "      nit: 1\n",
      "     njev: 30\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([1.29712636, 1.82362347, 1.82362347, 1.82362347, 1.82362347,\n",
      "       1.82362347, 1.82362347, 1.82362347, 1.82362347, 2.52649711])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Starting point: [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      "Nelder-Mead:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "Result:\n",
      " final_simplex: (array([[ 4.89014900e-01,  2.51113998e-01,  3.30060714e-02,\n",
      "         4.77126703e-02,  4.69882172e-02,  2.25431617e-02,\n",
      "        -2.02245929e-03, -4.64418087e+00,  2.20970204e+01,\n",
      "         4.88316448e+02],\n",
      "       [ 4.76428987e-01,  2.41688041e-01,  4.14773276e-02,\n",
      "         6.65893248e-02,  4.35233314e-02,  1.29780149e-02,\n",
      "        -1.08615392e-03, -4.64523022e+00,  2.20970615e+01,\n",
      "         4.88312997e+02],\n",
      "       [ 4.57673160e-01,  2.24195633e-01,  4.65206769e-02,\n",
      "         5.07830256e-02,  4.06301447e-02,  2.18378464e-02,\n",
      "        -7.72015423e-03, -4.64570356e+00,  2.20974393e+01,\n",
      "         4.88336416e+02],\n",
      "       [ 4.66747750e-01,  2.24963635e-01,  3.61565210e-02,\n",
      "         4.52821075e-02,  4.46821312e-02,  1.30364672e-02,\n",
      "        -9.87687613e-04, -4.64342812e+00,  2.20975162e+01,\n",
      "         4.88344498e+02],\n",
      "       [ 4.73672570e-01,  2.36159061e-01,  4.01688572e-02,\n",
      "         6.02324083e-02,  3.15137868e-02,  4.46978637e-02,\n",
      "         2.29996351e-03, -4.64428009e+00,  2.20968546e+01,\n",
      "         4.88316041e+02],\n",
      "       [ 4.68018851e-01,  2.23783092e-01,  3.85636586e-02,\n",
      "         6.84191281e-02,  2.78820161e-02,  1.81863072e-02,\n",
      "        -2.24256965e-03, -4.64456749e+00,  2.20973228e+01,\n",
      "         4.88334122e+02],\n",
      "       [ 4.95342574e-01,  2.27705169e-01,  3.48415301e-02,\n",
      "         6.72264480e-02,  3.67547061e-02,  1.43613737e-02,\n",
      "         8.79056200e-04, -4.64421857e+00,  2.20973354e+01,\n",
      "         4.88331525e+02],\n",
      "       [ 4.82226953e-01,  2.38175149e-01,  2.95241758e-02,\n",
      "         6.03948472e-02,  3.30727368e-02,  2.46010605e-02,\n",
      "        -6.31254534e-03, -4.64574575e+00,  2.20971353e+01,\n",
      "         4.88329974e+02],\n",
      "       [ 4.46388581e-01,  2.11147034e-01,  2.68798043e-02,\n",
      "         3.90517546e-02,  4.52282604e-02,  3.60010470e-02,\n",
      "         3.35795820e-03, -4.64582363e+00,  2.20981578e+01,\n",
      "         4.88362724e+02],\n",
      "       [ 4.70458197e-01,  2.35216093e-01,  3.70852550e-02,\n",
      "         6.04942105e-02,  5.58565378e-02,  1.61017530e-02,\n",
      "        -5.22748285e-03, -4.64502490e+00,  2.20972433e+01,\n",
      "         4.88324685e+02],\n",
      "       [ 4.48859451e-01,  2.02848925e-01,  4.24945665e-02,\n",
      "         6.64761175e-02,  3.57475298e-02,  2.64484265e-02,\n",
      "        -5.85963093e-03, -4.64543982e+00,  2.20982847e+01,\n",
      "         4.88352113e+02]]), array([2667.96965569, 2667.97238892, 2668.00466712, 2668.05990427,\n",
      "       2668.06581727, 2668.07886911, 2668.0920843 , 2668.10152005,\n",
      "       2668.10384297, 2668.12664838, 2668.1353697 ]))\n",
      "           fun: 2667.96965568853\n",
      "       message: 'Maximum number of function evaluations has been exceeded.'\n",
      "          nfev: 2001\n",
      "           nit: 1439\n",
      "        status: 1\n",
      "       success: False\n",
      "             x: array([ 4.89014900e-01,  2.51113998e-01,  3.30060714e-02,  4.77126703e-02,\n",
      "        4.69882172e-02,  2.25431617e-02, -2.02245929e-03, -4.64418087e+00,\n",
      "        2.20970204e+01,  4.88316448e+02])\n",
      "\n",
      "BFGS:\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 50360024165.476677\n",
      "         Iterations: 2\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 101\n",
      "Result:\n",
      "      fun: 50360024165.47668\n",
      " hess_inv: array([[ 0.09449433,  0.23561286, -0.10428015, -0.05330233, -0.05330233,\n",
      "        -0.05330233, -0.05330233, -0.05328614, -0.05611119,  0.05935912],\n",
      "       [ 0.23561286,  0.62189872, -0.11136559, -0.16648375, -0.16648375,\n",
      "        -0.16648375, -0.16648375, -0.16650126, -0.16563561,  0.06042067],\n",
      "       [-0.10428015, -0.11136559,  0.93840439, -0.08361933, -0.08361933,\n",
      "        -0.08361933, -0.08361933, -0.08362633, -0.08451451, -0.01707419],\n",
      "       [-0.05330233, -0.16648375, -0.08361933,  0.89435694, -0.10564306,\n",
      "        -0.10564306, -0.10564306, -0.10565005, -0.10581933,  0.03956576],\n",
      "       [-0.05330233, -0.16648375, -0.08361933, -0.10564306,  0.89435694,\n",
      "        -0.10564306, -0.10564306, -0.10565005, -0.10581933,  0.03956576],\n",
      "       [-0.05330233, -0.16648375, -0.08361933, -0.10564306, -0.10564306,\n",
      "         0.89435694, -0.10564306, -0.10565005, -0.10581933,  0.03956576],\n",
      "       [-0.05330233, -0.16648375, -0.08361933, -0.10564306, -0.10564306,\n",
      "        -0.10564306,  0.89435694, -0.10565005, -0.10581933,  0.03956576],\n",
      "       [-0.05328614, -0.16650126, -0.08362633, -0.10565005, -0.10565005,\n",
      "        -0.10565005, -0.10565005,  0.89434295, -0.1058261 ,  0.03958376],\n",
      "       [-0.05611119, -0.16563561, -0.08451451, -0.10581933, -0.10581933,\n",
      "        -0.10581933, -0.10581933, -0.1058261 ,  0.89403103,  0.04296499],\n",
      "       [ 0.05935912,  0.06042067, -0.01707419,  0.03956576,  0.03956576,\n",
      "         0.03956576,  0.03956576,  0.03958376,  0.04296499,  1.27508516]])\n",
      "      jac: array([ 7.27754679e+08, -9.77734528e+07,  1.45749758e+08,  9.77123080e+07,\n",
      "        9.77123080e+07,  9.77123080e+07,  9.77123080e+07,  9.76970457e+07,\n",
      "        9.86916094e+07, -1.83641858e+08])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 113\n",
      "      nit: 2\n",
      "     njev: 101\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([122.23567915,  57.31384693,  79.42816149,  79.42816149,\n",
      "        79.42816149,  79.42816149,  79.42816149,  79.42816149,\n",
      "        79.90854391, 143.82131939])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "first_guesses = [\n",
    "    # try a few starting points varying distances away\n",
    "    np.array([0.0] * 10),\n",
    "    np.array([1.0] * 10),\n",
    "    np.array([2.0] * 10),\n",
    "    np.array([100.0] * 10),\n",
    "]\n",
    "\n",
    "for x0 in first_guesses:\n",
    "    print('=' * 80)\n",
    "    print('')\n",
    "    print(f'Starting point: {x0}')\n",
    "    \n",
    "    print('Nelder-Mead:')\n",
    "    nm_result = minimize(\n",
    "        obj,\n",
    "        x0,\n",
    "        method='Nelder-Mead',\n",
    "        tol=1e-6,\n",
    "        options={'disp': True},\n",
    "    )\n",
    "    print('Result:')\n",
    "    print(nm_result)\n",
    "    print('')\n",
    "    \n",
    "    print('BFGS:')\n",
    "    bfgs_result = minimize(\n",
    "        obj,\n",
    "        x0,\n",
    "        method='BFGS',\n",
    "        jac=obj_gradient,\n",
    "        tol=1e-6,\n",
    "        options={'disp': True},\n",
    "    )\n",
    "    print('Result:')\n",
    "    print(bfgs_result)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods produce very different results depending on the chosen starting point. I think this is because both methods run out of allowed iterations before reaching convergence, and the function has a very flat minimal region (in the 2D case it's shaped like a parabola; in this case it's impossible to visualize but I assume it's something like a 10D paraboloid). Thus all points on this paraboloid-ish surface have objective function values very close to the minimum.\n",
    "\n",
    "Nelder-Mead seems to take thousands of iterations if we start from anywhere that isn't already in the minimum region. BFGS, on the other hand, takes an order of magnitude fewer function evaluations, which is expected as it uses gradient information to find good search directions. However, BFGS doesn't appear to find better results. On the contrary, in the last example of starting at (100, ..., 100), BFGS comes up with a much worse solution than Nelder-Mead. This seems strange, but it does look like the gradient at the point it finds is close to zero. This may be because the shape of the function is very flat, but I think I've probably made a mistake in my gradient formulation. I've looked over the calculations many times and I can't find a mistake, but this result seems strange otherwise.\n",
    "\n",
    "Note: I calculated the gradient by hand because I don't have easy access to the `ad` library with the way I've set up my OS and my workspace. Using that library would be an easy way to check the results.\n",
    "\n",
    "(1, ..., 1) looks like it's an optimal solution as both methods return it if we start there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to solve a black-box optimization problem where the objective and constraint function values can be obtained by calling an executable. The executable <i>prob3</i> will be available at the course website http://users.jyu.fi/~jhaka/opt/ along with instructions on how to use it in the <i>README</i> file. \n",
    "\n",
    "The format of the problem is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\ &f(x)\\\\\n",
    "\\text{s.t. }&h_1(x) = 0\\\\\n",
    "        &h_2(x) = 0\\\\\n",
    "        &g_1(x) \\geq 0\\\\\n",
    "        &g_2(x) \\geq 0\\\\\n",
    "        &g_3(x) \\geq 0\\\\\n",
    "        &g_4(x) \\geq 0\\\\\n",
    "        &x\\in \\mathbb R^4.        \n",
    "\\end{align}\n",
    "$$\n",
    "Solve the optimization problem by using the tools and optimization method of your choosing. **Analyse the results!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw output: [-6.5, -0.5, -1.5, 0.5, 1.0, 0.0, -2.0, -4.0, -3.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "f_val: -6.5\n",
      "c_vals: [-0.5, -1.5, 0.5, 1.0, 0.0, -2.0]\n",
      "f_gradient: [-4. -3.  0.  0.]\n",
      "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# setting up reading and writing the files used by the executable\n",
    "def _write_input(x):\n",
    "    assert(len(x) == 4)\n",
    "    infile = open(\"input.txt\", \"w\")\n",
    "    infile.write(\"\\n\".join([str(xi) for xi in x]))\n",
    "    infile.close()\n",
    "\n",
    "def _read_output():\n",
    "    outfile = open(\"output.txt\", \"r\")\n",
    "    lines = outfile.readlines()\n",
    "    outfile.close()\n",
    "    return [float(line) for line in lines]\n",
    "\n",
    "def run_eval(x):\n",
    "    _write_input(x)\n",
    "    subprocess.run([\"./prob3_linux\"])\n",
    "    return _read_output()\n",
    "\n",
    "# names for ranges where things are in the output\n",
    "def f_val(output):\n",
    "    return output[0]\n",
    "\n",
    "def c_vals(output):\n",
    "    return output[1:7]\n",
    "\n",
    "def f_gradient(output):\n",
    "    return np.array(output[7:11])\n",
    "\n",
    "def c_gradients(output):\n",
    "    return [np.array(output[i:i+4]) for i in range(11, 35, 4)]\n",
    "\n",
    "c_types = ['eq', 'eq', 'ineq', 'ineq', 'ineq', 'ineq']\n",
    "\n",
    "# test that it works\n",
    "test_output = run_eval(np.array([0.5, 1.0, 0.0, -2.0]))\n",
    "def print_output(output):\n",
    "    print(f\"raw output: {output}\")\n",
    "    print(f\"f_val: {f_val(output)}\")\n",
    "    print(f\"c_vals: {c_vals(output)}\")\n",
    "    print(f\"f_gradient: {f_gradient(output)}\")\n",
    "    print(f\"c_gradients: {c_gradients(output)}\")\n",
    "print_output(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Starting point: [0. 0. 0. 0.]\n",
      "raw output: [0.0, -2.0, -5.0, 0.0, 0.0, 0.0, 0.0, -4.0, -6.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "f_val: 0.0\n",
      "c_vals: [-2.0, -5.0, 0.0, 0.0, 0.0, 0.0]\n",
      "f_gradient: [-4. -6.  0.  0.]\n",
      "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n",
      "Result:\n",
      "Singular matrix C in LSQ subproblem    (Exit mode 6)\n",
      "            Current function value: 0.0\n",
      "            Iterations: 1\n",
      "            Function evaluations: 1\n",
      "            Gradient evaluations: 1\n",
      "     fun: 0.0\n",
      "     jac: array([-4., -6.,  0.,  0.])\n",
      " message: 'Singular matrix C in LSQ subproblem'\n",
      "    nfev: 1\n",
      "     nit: 1\n",
      "    njev: 1\n",
      "  status: 6\n",
      " success: False\n",
      "       x: array([0., 0., 0., 0.])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Starting point: [1. 1. 1. 1.]\n",
      "raw output: [-8.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, -2.0, -4.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "f_val: -8.0\n",
      "c_vals: [1.0, 2.0, 1.0, 1.0, 1.0, 1.0]\n",
      "f_gradient: [-2. -4.  0.  0.]\n",
      "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n",
      "Result:\n",
      "Singular matrix C in LSQ subproblem    (Exit mode 6)\n",
      "            Current function value: -8.0\n",
      "            Iterations: 1\n",
      "            Function evaluations: 1\n",
      "            Gradient evaluations: 1\n",
      "     fun: -8.0\n",
      "     jac: array([-2., -4.,  0.,  0.])\n",
      " message: 'Singular matrix C in LSQ subproblem'\n",
      "    nfev: 1\n",
      "     nit: 1\n",
      "    njev: 1\n",
      "  status: 6\n",
      " success: False\n",
      "       x: array([1., 1., 1., 1.])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Starting point: [-2.   1.  -3.   0.5]\n",
      "raw output: [16.0, -6.0, -1.5, -2.0, 1.0, -3.0, 0.5, -14.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "f_val: 16.0\n",
      "c_vals: [-6.0, -1.5, -2.0, 1.0, -3.0, 0.5]\n",
      "f_gradient: [-14.   2.   0.   0.]\n",
      "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n",
      "Result:\n",
      "Singular matrix C in LSQ subproblem    (Exit mode 6)\n",
      "            Current function value: 16.0\n",
      "            Iterations: 1\n",
      "            Function evaluations: 1\n",
      "            Gradient evaluations: 1\n",
      "     fun: 16.0\n",
      "     jac: array([-14.,   2.,   0.,   0.])\n",
      " message: 'Singular matrix C in LSQ subproblem'\n",
      "    nfev: 1\n",
      "     nit: 1\n",
      "    njev: 1\n",
      "  status: 6\n",
      " success: False\n",
      "       x: array([-2. ,  1. , -3. ,  0.5])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Starting point: [ 3.00e+01 -1.30e+00 -5.08e+01  5.00e-04]\n",
      "raw output: [1769.180054, -24.099998, 18.5005, 30.0, -1.3, -50.799999, 0.0005, 118.599998, -71.199997, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "f_val: 1769.180054\n",
      "c_vals: [-24.099998, 18.5005, 30.0, -1.3, -50.799999, 0.0005]\n",
      "f_gradient: [118.599998 -71.199997   0.         0.      ]\n",
      "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n",
      "Result:\n",
      "Singular matrix C in LSQ subproblem    (Exit mode 6)\n",
      "            Current function value: 1769.180054\n",
      "            Iterations: 1\n",
      "            Function evaluations: 1\n",
      "            Gradient evaluations: 1\n",
      "     fun: 1769.180054\n",
      "     jac: array([118.599998, -71.199997,   0.      ,   0.      ])\n",
      " message: 'Singular matrix C in LSQ subproblem'\n",
      "    nfev: 1\n",
      "     nit: 1\n",
      "    njev: 1\n",
      "  status: 6\n",
      " success: False\n",
      "       x: array([ 3.00e+01, -1.30e+00, -5.08e+01,  5.00e-04])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# time to actually solve it\n",
    "\n",
    "# looks like scipy.optimize.minimize has a `callback` parameter\n",
    "# we can use to run the program between iterations. Let's see if this works.\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def solve_scipy(x0):\n",
    "    # store program output in a variable which we'll update between iterations\n",
    "    output = run_eval(x0)\n",
    "    def update_output(x):\n",
    "        output = run_eval(x)\n",
    "        \n",
    "    result = minimize(\n",
    "        lambda _: f_val(output),\n",
    "        x0,\n",
    "        # SLSQP because it can handle constraints and uses gradients,\n",
    "        # which we have available\n",
    "        method='SLSQP',\n",
    "        jac=lambda _: f_gradient(output),\n",
    "        constraints=[\n",
    "            {'type': c_type, 'fun': lambda _: c_val, 'jac': lambda _: c_grad}\n",
    "            for (c_val, c_grad, c_type)\n",
    "            in zip(c_vals(output), c_gradients(output), c_types)\n",
    "        ],\n",
    "        callback=update_output,\n",
    "        options={'disp': True},\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# test the method\n",
    "\n",
    "def test_solver(solver):\n",
    "    first_guesses = [\n",
    "        # like in task 2, try a few starting points varying distances away from the origin\n",
    "        np.array([0.0] * 4),\n",
    "        np.array([1.0] * 4),\n",
    "        np.array([-2.0, 1.0, -3.0, 0.5]),\n",
    "        np.array([30.0, -1.3, -50.8, 0.0005]),\n",
    "    ]\n",
    "    for x0 in first_guesses:\n",
    "        print('=' * 80)\n",
    "        print('')\n",
    "        print(f'Starting point: {x0}')\n",
    "        print_output(run_eval(x0))\n",
    "        print('Result:')\n",
    "        result = solver(x0)\n",
    "        print(result)\n",
    "        print('')\n",
    "\n",
    "test_solver(solve_scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method runs into a non-invertible matrix in every case. I'm not sure why. It could be because the scipy method is doing e.g. some finite differencing and getting zero differences because we're only updating the objective and constraint values between iterations. Let's try a terribly inefficient approach that re-runs the program for every evaluation of every function, just to be sure the problem isn't with an implementation detail.\n",
    "\n",
    "(Note: just in case I somehow have an incorrect version of the problem executable and the outputs are different for you, here's an example of what the above prints on my machine:)\n",
    "```\n",
    "Starting point: [0. 0. 0. 0.]\n",
    "raw output: [0.0, -2.0, -5.0, 0.0, 0.0, 0.0, 0.0, -4.0, -6.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
    "f_val: 0.0\n",
    "c_vals: [-2.0, -5.0, 0.0, 0.0, 0.0, 0.0]\n",
    "f_gradient: [-4. -6.  0.  0.]\n",
    "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n",
    "Result:\n",
    "Singular matrix C in LSQ subproblem    (Exit mode 6)\n",
    "            Current function value: 0.0\n",
    "            Iterations: 1\n",
    "            Function evaluations: 1\n",
    "            Gradient evaluations: 1\n",
    "     fun: 0.0\n",
    "     jac: array([-4., -6.,  0.,  0.])\n",
    " message: 'Singular matrix C in LSQ subproblem'\n",
    "    nfev: 1\n",
    "     nit: 1\n",
    "    njev: 1\n",
    "  status: 6\n",
    " success: False\n",
    "       x: array([0., 0., 0., 0.])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Starting point: [0. 0. 0. 0.]\n",
      "raw output: [0.0, -2.0, -5.0, 0.0, 0.0, 0.0, 0.0, -4.0, -6.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "f_val: 0.0\n",
      "c_vals: [-2.0, -5.0, 0.0, 0.0, 0.0, 0.0]\n",
      "f_gradient: [-4. -6.  0.  0.]\n",
      "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n",
      "Result:\n",
      "Singular matrix C in LSQ subproblem    (Exit mode 6)\n",
      "            Current function value: 0.0\n",
      "            Iterations: 1\n",
      "            Function evaluations: 1\n",
      "            Gradient evaluations: 1\n",
      "     fun: 0.0\n",
      "     jac: array([-4., -6.,  0.,  0.])\n",
      " message: 'Singular matrix C in LSQ subproblem'\n",
      "    nfev: 1\n",
      "     nit: 1\n",
      "    njev: 1\n",
      "  status: 6\n",
      " success: False\n",
      "       x: array([0., 0., 0., 0.])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Starting point: [1. 1. 1. 1.]\n",
      "raw output: [-8.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, -2.0, -4.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "f_val: -8.0\n",
      "c_vals: [1.0, 2.0, 1.0, 1.0, 1.0, 1.0]\n",
      "f_gradient: [-2. -4.  0.  0.]\n",
      "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n",
      "Result:\n",
      "Singular matrix C in LSQ subproblem    (Exit mode 6)\n",
      "            Current function value: -8.0\n",
      "            Iterations: 1\n",
      "            Function evaluations: 1\n",
      "            Gradient evaluations: 1\n",
      "     fun: -8.0\n",
      "     jac: array([-2., -4.,  0.,  0.])\n",
      " message: 'Singular matrix C in LSQ subproblem'\n",
      "    nfev: 1\n",
      "     nit: 1\n",
      "    njev: 1\n",
      "  status: 6\n",
      " success: False\n",
      "       x: array([1., 1., 1., 1.])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Starting point: [-2.   1.  -3.   0.5]\n",
      "raw output: [16.0, -6.0, -1.5, -2.0, 1.0, -3.0, 0.5, -14.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "f_val: 16.0\n",
      "c_vals: [-6.0, -1.5, -2.0, 1.0, -3.0, 0.5]\n",
      "f_gradient: [-14.   2.   0.   0.]\n",
      "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n",
      "Result:\n",
      "Singular matrix C in LSQ subproblem    (Exit mode 6)\n",
      "            Current function value: 16.0\n",
      "            Iterations: 1\n",
      "            Function evaluations: 1\n",
      "            Gradient evaluations: 1\n",
      "     fun: 16.0\n",
      "     jac: array([-14.,   2.,   0.,   0.])\n",
      " message: 'Singular matrix C in LSQ subproblem'\n",
      "    nfev: 1\n",
      "     nit: 1\n",
      "    njev: 1\n",
      "  status: 6\n",
      " success: False\n",
      "       x: array([-2. ,  1. , -3. ,  0.5])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Starting point: [ 3.00e+01 -1.30e+00 -5.08e+01  5.00e-04]\n",
      "raw output: [1769.180054, -24.099998, 18.5005, 30.0, -1.3, -50.799999, 0.0005, 118.599998, -71.199997, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "f_val: 1769.180054\n",
      "c_vals: [-24.099998, 18.5005, 30.0, -1.3, -50.799999, 0.0005]\n",
      "f_gradient: [118.599998 -71.199997   0.         0.      ]\n",
      "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n",
      "Result:\n",
      "Singular matrix C in LSQ subproblem    (Exit mode 6)\n",
      "            Current function value: 1769.180054\n",
      "            Iterations: 1\n",
      "            Function evaluations: 1\n",
      "            Gradient evaluations: 1\n",
      "     fun: 1769.180054\n",
      "     jac: array([118.599998, -71.199997,   0.      ,   0.      ])\n",
      " message: 'Singular matrix C in LSQ subproblem'\n",
      "    nfev: 1\n",
      "     nit: 1\n",
      "    njev: 1\n",
      "  status: 6\n",
      " success: False\n",
      "       x: array([ 3.00e+01, -1.30e+00, -5.08e+01,  5.00e-04])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def solve_scipy_safe(x0):\n",
    "    result = minimize(\n",
    "        lambda x: f_val(run_eval(x)),\n",
    "        x0,\n",
    "        # SLSQP because it can handle constraints and uses gradients,\n",
    "        # which we have available\n",
    "        method='SLSQP',\n",
    "        jac=lambda x: f_gradient(run_eval(x)),\n",
    "        constraints=[\n",
    "            {\n",
    "                'type': c_types[i],\n",
    "                'fun': lambda x: c_vals(run_eval(x))[i],\n",
    "                'jac': lambda x: c_gradients(run_eval(x))[i]\n",
    "            }\n",
    "            for i\n",
    "            in range(0, len(c_types))\n",
    "        ],\n",
    "        options={'disp': True},\n",
    "    )\n",
    "    return result\n",
    "\n",
    "test_solver(solve_scipy_safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still getting the same function and gradient values and the same errors, so the problem is likely that the SLSQP method is incompatible with this task. Perhaps it's because the gradient of the objective function is zero for the last two variables in every case.\n",
    "\n",
    "Examining the gradient values gives us some more information here. It looks like all of the constraints are linear (their gradients are constant). Also, all the inequality constraints only depend on one variable and have the value 0 at 0 with a gradient of 1 everywhere. Therefore, we can deduce (by integration) that the inequality constraints are\n",
    "$g_i(x) = x_i \\text{ for all } i = 1, \\dots, 4$, which can be translated to the bounds $x_i \\geq 0$. Since the gradient of the objective function is 0 with respect to the last two variables, and thus the function is constant w.r.t. them, we can reduce the problem to a linear problem with two decision variables and two linear equality constraints, bounded to the non-negative reals.\n",
    "\n",
    "Let's first see if the SLSQP method from scipy.optimize.minimize can handle this simplified problem. If not, this is simple enough that we could implement a custom projected gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "Starting point: [0. 0. 0. 0.]\n",
      "raw output: [0.0, -2.0, -5.0, 0.0, 0.0, 0.0, 0.0, -4.0, -6.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "f_val: 0.0\n",
      "c_vals: [-2.0, -5.0, 0.0, 0.0, 0.0, 0.0]\n",
      "f_gradient: [-4. -6.  0.  0.]\n",
      "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n",
      "Result:\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -7.125\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "     fun: -7.125\n",
      "     jac: array([-0.5, -5.5])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 2\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1.25, 0.75])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Starting point: [1. 1. 1. 1.]\n",
      "raw output: [-8.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, -2.0, -4.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "f_val: -8.0\n",
      "c_vals: [1.0, 2.0, 1.0, 1.0, 1.0, 1.0]\n",
      "f_gradient: [-2. -4.  0.  0.]\n",
      "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n",
      "Result:\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -7.125\n",
      "            Iterations: 3\n",
      "            Function evaluations: 4\n",
      "            Gradient evaluations: 3\n",
      "     fun: -7.125\n",
      "     jac: array([-0.500001, -5.5     ])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 4\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1.24999992, 0.75000008])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Starting point: [-2.   1.  -3.   0.5]\n",
      "raw output: [16.0, -6.0, -1.5, -2.0, 1.0, -3.0, 0.5, -14.0, 2.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "f_val: 16.0\n",
      "c_vals: [-6.0, -1.5, -2.0, 1.0, -3.0, 0.5]\n",
      "f_gradient: [-14.   2.   0.   0.]\n",
      "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n",
      "Result:\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -7.125\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "     fun: -7.125\n",
      "     jac: array([-0.5, -5.5])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 2\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1.25, 0.75])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Starting point: [ 3.00e+01 -1.30e+00 -5.08e+01  5.00e-04]\n",
      "raw output: [1769.180054, -24.099998, 18.5005, 30.0, -1.3, -50.799999, 0.0005, 118.599998, -71.199997, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 5.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "f_val: 1769.180054\n",
      "c_vals: [-24.099998, 18.5005, 30.0, -1.3, -50.799999, 0.0005]\n",
      "f_gradient: [118.599998 -71.199997   0.         0.      ]\n",
      "c_gradients: [array([1., 1., 1., 0.]), array([1., 5., 0., 1.]), array([1., 0., 0., 0.]), array([0., 1., 0., 0.]), array([0., 0., 1., 0.]), array([0., 0., 0., 1.])]\n",
      "Result:\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -7.125\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "     fun: -7.125\n",
      "     jac: array([-0.5, -5.5])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 2\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1.25, 0.75])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def solve_scipy_simplified(x0):\n",
    "    def pad_x(x):\n",
    "        \"\"\"Pad x to a 4D vector for evaluating the 4D program.\"\"\"\n",
    "        return [x[0], x[1], 0, 0]\n",
    "    \n",
    "    result = minimize(\n",
    "        lambda x: f_val(run_eval(pad_x(x))),\n",
    "        # only take the first two variables of everything\n",
    "        x0[0:2],\n",
    "        method='SLSQP',\n",
    "        jac=lambda x: f_gradient(run_eval(pad_x(x)))[0:2],\n",
    "        constraints=[\n",
    "            {'type': 'eq',\n",
    "             'fun': lambda x: c_vals(run_eval(pad_x(x)))[0],\n",
    "             'jac': lambda x: c_gradients(run_eval(pad_x(x)))[0][0:2]},\n",
    "            {'type': 'eq',\n",
    "             'fun': lambda x: c_vals(run_eval(pad_x(x)))[1],\n",
    "             'jac': lambda x: c_gradients(run_eval(pad_x(x)))[1][0:2]},\n",
    "        ],\n",
    "        bounds=[\n",
    "            (0, None),\n",
    "            (0, None),\n",
    "        ],\n",
    "        options={'disp': True},\n",
    "    )\n",
    "    return result\n",
    "\n",
    "test_solver(solve_scipy_simplified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This successfully finds (1.25, 0.75) from every starting point! If the assumption that the gradient of $f$ is zero for all $x_3$ and $x_4$ is true, we should be able to give any values of those variables and get the same value of $f$. Picking zeroes for them, we get a solution $x^* = (1.25, 0.75, 0, 0)$.\n",
    "\n",
    "There's a chance that this solution is incorrect: if some of the functions are discontinuous, and we simply happened to sample only points where the gradients are constant, we could have missed a better solution, or the objective function could actually vary on $x_3$ or $x_4$ in some region of $\\mathbb{R}^4$. This seems unlikely to me, so I'm satisfied with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study biobjective optimization problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\ &(\\|x-(1,0)\\|,\\|x-(0,1)\\|)\\\\\n",
    "\\text{s.t. }&x\\in \\mathbb R^2.\n",
    "\\end{align}\n",
    "$$\n",
    "Try to generate an evenly spread representation of the Pareto front. Plot the results in both the decision and objective spaces. **Analyze the results!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a lot like the problem we explored in exercise 6 of the course exercises. The only difference is that the exercise measured the distances squared, and this one does linearly. The weighting method performed fairly well in the exercise, so let's try using it with this problem as well. This code is copied from that exercise with modifications to the objective functions and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/bzbi2yq42fzsi1p0jb86pj9wq4mbapc4-python3.7-ipykernel-5.1.4/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/nix/store/bzbi2yq42fzsi1p0jb86pj9wq4mbapc4-python3.7-ipykernel-5.1.4/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAF4CAYAAACl5tU8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAARSUlEQVR4nO3df6jl9X3n8de7OqElZmsXBxR/JruytmloYgZjCCzubgtqAy5s/rB/JODCDnEtNaX9Q/JHst2/WlgCa1IcBiJpSjalkDRIoin5IyXJshpHUaOxLbPdbp0qOEmoRhLStfveP+4R7l7v+L73znzn/ujjAYc553w/59z3ly/O0+85556p7g4AvJGf2u0BANj7xAKAkVgAMBILAEZiAcBILAAYLRqLqvrpqvp2VT1ZVc9U1e9ssqaq6t6qOllVT1XV9UvOBMD2Xbjw8/8kyb/u7leq6lCSb1XVQ9398Lo1tyS5dnV5T5L7Vn8CsEcsembRa15Z3Ty0umz8LcDbknx2tfbhJBdX1WVLzgXA9iz+nkVVXVBVTyR5McnXuvuRDUsuT/LcutunVvcBsEcs/TJUuvsfkryzqi5O8idV9Yvd/fS6JbXZwzbeUVVHkxxNkje/+c3vvu666xaZF+Cgeuyxx77X3Yd38tjFY/Ga7v67qvqzJDcnWR+LU0muXHf7iiTPb/L440mOJ8mRI0f6xIkTyw0LcABV1f/e6WOX/jTU4dUZRarqZ5L8cpI/37DsgSQfWn0q6sYkL3X3C0vOBcD2LH1mcVmSP6iqC7IWpj/u7i9X1YeTpLuPJXkwya1JTib5UZI7Fp4JgG1aNBbd/VSSd21y/7F11zvJXUvOAcDZ8RvcAIzEAoCRWAAwEgsARmIBwEgsABiJBQAjsQBgJBYAjMQCgJFYADASCwBGYgHASCwAGIkFACOxAGAkFgCMxAKAkVgAMBILAEZiAcBILAAYiQUAI7EAYCQWAIzEAoCRWAAwEgsARmIBwEgsABiJBQAjsQBgJBYAjMQCgJFYADASCwBGYgHASCwAGIkFACOxAGAkFgCMxAKAkVgAMBILAEZiAcBILAAYLRqLqrqyqr5eVc9W1TNVdfcma26qqpeq6onV5WNLzgTA9l248PO/muS3uvvxqnpLkseq6mvd/d0N677Z3e9feBYAdmjRM4vufqG7H19d/2GSZ5NcvuTPBODcO2/vWVTVNUneleSRTTa/t6qerKqHqurtZ3j80ao6UVUnTp8+veCkAGx0XmJRVRcl+UKSj3T3yxs2P57k6u7+pSSfTPKlzZ6ju49395HuPnL48OFlBwbg/7N4LKrqUNZC8bnu/uLG7d39cne/srr+YJJDVXXJ0nMBsHVLfxqqknw6ybPd/YkzrLl0tS5VdcNqpu8vORcA27P0p6Hel+SDSb5TVU+s7vtokquSpLuPJflAkjur6tUkP05ye3f3wnMBsA2LxqK7v5WkhjWfSvKpJecA4Oz4DW4ARmIBwEgsABiJBQAjsQBgJBYAjMQCgJFYADASCwBGYgHASCwAGIkFACOxAGAkFgCMxAKAkVgAMBILAEZiAcBILAAYiQUAI7EAYCQWAIzEAoCRWAAwEgsARmIBwEgsABiJBQAjsQBgJBYAjMQCgJFYADASCwBGYgHASCwAGIkFACOxAGAkFgCMxAKAkVgAMBILAEZiAcBILAAYiQUAI7EAYLRoLKrqyqr6elU9W1XPVNXdm6ypqrq3qk5W1VNVdf2SMwGwfRcu/PyvJvmt7n68qt6S5LGq+lp3f3fdmluSXLu6vCfJfas/AdgjFj2z6O4Xuvvx1fUfJnk2yeUblt2W5LO95uEkF1fVZUvOBcD2nLf3LKrqmiTvSvLIhk2XJ3lu3e1TeX1QUlVHq+pEVZ04ffr0UmMCsInzEouquijJF5J8pLtf3rh5k4f06+7oPt7dR7r7yOHDh5cYE4AzWDwWVXUoa6H4XHd/cZMlp5Jcue72FUmeX3ouALZu6U9DVZJPJ3m2uz9xhmUPJPnQ6lNRNyZ5qbtfWHIuALZn6U9DvS/JB5N8p6qeWN330SRXJUl3H0vyYJJbk5xM8qMkdyw8EwDbtGgsuvtb2fw9ifVrOsldS84BwNnxG9wAjMQCgJFYADASCwBGYgHASCwAGIkFACOxAGAkFgCMxAKAkVgAMBILAEZiAcBILAAYiQUAI7EAYCQWAIzEAoCRWAAwEgsARmIBwEgsABiJBQAjsQBgJBYAjMQCgJFYADASCwBGYgHASCwAGIkFACOxAGAkFgCMxAKAkVgAMBILAEZiAcBILAAYiQUAI7EAYCQWAIzEAoCRWAAwEgsARmIBwOgNY1FV76iqh6vquao6XlU/t27bt6cnr6r7q+rFqnr6DNtvqqqXquqJ1eVj298FAJY2nVncl+Q/JXlHkr9M8q2q+merbYe28PyfSXLzsOab3f3O1eU/b+E5ATjPLhy2X9TdX11d/y9V9ViSr1bVB5P09OTd/Y2quubsRgRgt01nFlVVP/vaje7+epJ/l+QPk1x9jmZ4b1U9WVUPVdXbz9FzAnAOTbH4vSQ/v/6O7n4qyb9J8sVz8PMfT3J1d/9Skk8m+dKZFlbV0ao6UVUnTp8+fQ5+NABb9Yax6O7/1t0Pb3L/33T3f3jtdlV9cic/vLtf7u5XVtcfTHKoqi45w9rj3X2ku48cPnx4Jz8OgB06Vx+dfd9OHlRVl1ZVra7fsJrn++doJgDOkekN7rNSVZ9PclOSS6rqVJKPZ/Upqu4+luQDSe6sqleT/DjJ7d09vnEOwPm1aCy6+9eG7Z9K8qklZwDg7E2/lPeHqz/vHp6nztlEAOw503sW766qq5P8+6r6uar6p+sv69b91wVnfJ3v/O1Lueaer+Sae75yPn8swD9aUyyOJflqkuuSPLbhcuK1Rd39mYXmGwkGwPKmj87e290/n+T+7n5bd7913eVt52lGAHbZlj462913Lj0IAHuXrygHYCQWAIz2fSz++nd/dbdHADjwFv2lvKW84/KfzQmRADhv9v2ZBQDLEwsARmIBwEgsABiJBQAjsQBgJBYAjMQCgJFYADASCwBGYgHASCwAGIkFACOxAGAkFgCMxAKAkVgAMBILAEZiAcBILAAYiQUAI7EAYCQWAIzEAoCRWAAwEgsARmIBwEgsABiJBQAjsQBgJBYAjMQCgJFYADASCwBGYgHASCwAGIkFAKNFY1FV91fVi1X19Bm2V1XdW1Unq+qpqrp+yXkA2Jmlzyw+k+TmN9h+S5JrV5ejSe5beB4AdmDRWHT3N5L84A2W3Jbks73m4SQXV9VlS84EwPbt9nsWlyd5bt3tU6v7XqeqjlbViao6cfr06fMyHABrdjsWtcl9vdnC7j7e3Ue6+8jhw4cXHguA9XY7FqeSXLnu9hVJnt+lWQA4g92OxQNJPrT6VNSNSV7q7hd2eSYANrhwySevqs8nuSnJJVV1KsnHkxxKku4+luTBJLcmOZnkR0nuWHIeAHZm0Vh0968N2zvJXUvOAMDZ2+2XoQDYB8QCgJFYADASCwBGYgHASCwAGIkFACOxAGAkFgCMxAKAkVgAMBILAEZiAcBILAAYiQUAI7EAYCQWAIzEAoCRWAAwEgsARmIBwEgsABiJBQAjsQBgJBYAjMQCgJFYADASCwBGYgHASCwAGIkFACOxAGAkFgCMxAKAkVgAMBILAEZiAcBILAAYiQUAI7EAYCQWAIzEAoCRWAAwEgsARmIBwGjxWFTVzVX1F1V1sqru2WT7TVX1UlU9sbp8bOmZANieC5d88qq6IMnvJ/mVJKeSPFpVD3T3dzcs/WZ3v3/JWQDYuaXPLG5IcrK7/6q7/z7JHyW5beGfCcA5tnQsLk/y3Lrbp1b3bfTeqnqyqh6qqrcvPBMA27Toy1BJapP7esPtx5Nc3d2vVNWtSb6U5NrXPVHV0SRHk+Sqq64613MC8AaWPrM4leTKdbevSPL8+gXd/XJ3v7K6/mCSQ1V1ycYn6u7j3X2ku48cPnx4yZkB2GDpWDya5NqqemtVvSnJ7UkeWL+gqi6tqlpdv2E10/cXnguAbVj0ZajufrWqfj3Jnya5IMn93f1MVX14tf1Ykg8kubOqXk3y4yS3d/fGl6oA2EW1H/9ePnLkSJ84cWK3xwDYV6rqse4+spPH+g1uAEZiAcBILAAYiQUAI7EAYCQWAIzEAoCRWAAwEgsARmIBwEgsABiJBQAjsQBgJBYAjMQCgJFYADASCwBGYgHASCwAGIkFACOxAGAkFgCMxAKAkVgAMBILAEZiAcBILAAYiQUAI7EAYCQWAIzEAoCRWAAwEgsARmIBwEgsABiJBQAjsQBgJBYAjMQCgJFYADASCwBGYgHASCwAGIkFACOxAGC0eCyq6uaq+ouqOllV92yyvarq3tX2p6rq+qVnAmB7LlzyyavqgiS/n+RXkpxK8mhVPdDd31237JYk164u70ly3+pP2BOuuecrr7vvr3/3V3dhEtg9S59Z3JDkZHf/VXf/fZI/SnLbhjW3Jflsr3k4ycVVddnCc8GWbBaKN7ofDqqlY3F5kufW3T61um+7awDYRYu+DJWkNrmvd7AmVXU0ydHVzZ9U1dNnOdtedkmS7+32EAvaN/v3pkv/+bvPtK1+7/2PnWHTvtm/HTjI+5Yc/P37Fzt94NKxOJXkynW3r0jy/A7WpLuPJzmeJFV1oruPnNtR9w77t78d5P07yPuW/OPYv50+dumXoR5Ncm1VvbWq3pTk9iQPbFjzQJIPrT4VdWOSl7r7hYXnAmAbFj2z6O5Xq+rXk/xpkguS3N/dz1TVh1fbjyV5MMmtSU4m+VGSO5acCYDtW/plqHT3g1kLwvr7jq273knu2ubTHj8Ho+1l9m9/O8j7d5D3LbF/Z1Rrf1cDwJn5ug8ARns6Fgf9q0K2sH83VdVLVfXE6vKx3ZhzJ6rq/qp68UwfcT4Ax27av/187K6sqq9X1bNV9UxV3b3Jmn17/La4f/v5+P10VX27qp5c7d/vbLJm+8evu/fkJWtviP/PJG9L8qYkTyb5hQ1rbk3yUNZ+V+PGJI/s9tzneP9uSvLl3Z51h/v3L5Ncn+TpM2zft8dui/u3n4/dZUmuX11/S5K/PGD/7W1l//bz8askF62uH0rySJIbz/b47eUzi4P+VSFb2b99q7u/keQHb7BkPx+7rezfvtXdL3T346vrP0zybF7/rQr79vhtcf/2rdUxeWV189DqsvHN6W0fv70ci4P+VSFbnf29q9PJh6rq7edntPNiPx+7rdr3x66qrknyrqz93+l6B+L4vcH+Jfv4+FXVBVX1RJIXk3ytu8/6+C3+0dmzcM6+KmSP2srsjye5urtfqapbk3wpa9/OexDs52O3Ffv+2FXVRUm+kOQj3f3yxs2bPGRfHb9h//b18evuf0jyzqq6OMmfVNUvdvf699e2ffz28pnFOfuqkD1qnL27X37tdLLXfl/lUFVdcv5GXNR+Pnaj/X7squpQ1v4i/Vx3f3GTJfv6+E37t9+P32u6+++S/FmSmzds2vbx28uxOOhfFTLuX1VdWlW1un5D1o7X98/7pMvYz8dutJ+P3WruTyd5trs/cYZl+/b4bWX/9vnxO7w6o0hV/UySX07y5xuWbfv47dmXofqAf1XIFvfvA0nurKpXk/w4ye29+ijDXldVn8/aJ0ouqapTST6etTfa9v2xS7a0f/v22CV5X5IPJvnO6nXvJPlokquSA3H8trJ/+/n4XZbkD2rtH5/7qSR/3N1fPtu/O/0GNwCjvfwyFAB7hFgAMBILAEZiAcBILAAYiQUAI7GAHaiq31h9xfXnNtl2XVX9j6r6SVX99m7MB+fanv2lPNjj/mOSW7r7f22y7QdJfiPJvz2/I8FynFnANlXVsaz9OyQPVNVvbtze3S9296NJ/s95Hw4W4swCtqm7P1xVNyf5V939vd2eB84HZxYAjMQCgJFYADDyngWcY1V1aZITSf5Jkv9bVR9J8gub/GtssG/4inIARl6GAmDkZSjYoaq6I8ndG+7+7919127MA0vyMhQAIy9DATASCwBGYgHASCwAGIkFAKP/B7m51/WbgAtnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "objectives = [\n",
    "    lambda x: sqrt((x[0] - 1)**2 + x[1]**2),\n",
    "    lambda x: sqrt(x[0]**2 + (x[1] - 1)**2),\n",
    "]\n",
    "\n",
    "obj_jacobians = [\n",
    "    # there's a division by zero error causing a warning when\n",
    "    # we're exactly on one of the points, but this doesn't seem to\n",
    "    # cause problems so I just ignore the warning\n",
    "    lambda x: np.array([(2*x[0] - 2) / (2*objectives[0](x)), (2*x[1]) / (2*objectives[0](x))]),\n",
    "    lambda x: np.array([2*x[0] / (2*objectives[1](x)), (2*x[1] - 2) / (2*objectives[1](x))]),\n",
    "]\n",
    "\n",
    "def weighted_objective(weights, x):\n",
    "    return sum([w * obj(x) for (w, obj) in zip(weights, objectives)])\n",
    "\n",
    "def weighted_jacobian(weights, x):\n",
    "    return sum([w * jac(x) for (w, jac) in zip(weights, obj_jacobians)])\n",
    "\n",
    "def gen_points_weighted(count):\n",
    "    \"\"\"Generate a set of Pareto optimal points\n",
    "    using a random set of weights.\n",
    "    \"\"\"\n",
    "    weights = np.random.random((count, 2))\n",
    "    \n",
    "    def find_point(weight):\n",
    "        return minimize(\n",
    "            fun=lambda x: weighted_objective(weight, x),\n",
    "            x0=[1, 1],\n",
    "            jac=lambda x: weighted_jacobian(weight, x),\n",
    "            method='SLSQP',\n",
    "            bounds=((0, 2), (0, 2)),\n",
    "            options={\"disp\": False, \"ftol\": 1e-20, \"maxiter\": 1000},\n",
    "        ).x\n",
    "        \n",
    "    return [find_point(w) for w in weights]\n",
    "\n",
    "# plot a set of points generated by the above\n",
    "\n",
    "points = gen_points_weighted(100)\n",
    "obj_values = [[obj(x) for x in points] for obj in objectives]\n",
    "\n",
    "# decision space\n",
    "plt.figure(figsize=[6, 6])\n",
    "plt.xlim([-1, 2])\n",
    "plt.xlabel('x_1')\n",
    "plt.ylim([-1, 2])\n",
    "plt.ylabel('x_2')\n",
    "\n",
    "plt.plot([x[0] for x in points], [x[1] for x in points], 'o')\n",
    "\n",
    "# objective space\n",
    "plt.figure(figsize=[6, 6])\n",
    "plt.xlim([0, 3])\n",
    "plt.xlabel('f_1')\n",
    "plt.ylim([0, 3])\n",
    "plt.ylabel('f_2')\n",
    "\n",
    "plt.plot(obj_values[0], obj_values[1], 'o')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out this method only finds the end points of the Pareto front, which tells us that the front isn't convex, but either linear or concave. Instead, in this case we can find the set of solutions analytically. The reasoning for this was already covered in exercise 6, but to review: For any given values of $f_1$ and $f_2$, the corresponding sets in the decision space are circles around the points $P_1 = (1, 0)$ and $P_2 = (0, 1)$ respectively. For a value of $f_1$, the lowest possible value of $f_2$ is the radius of the smallest circle that intersects with the circle defined by $f_1$. The intersection point is on the line between $P_1$ and $P_2$. The Pareto optimal case where neither circle can be made smaller without enlarging the other occurs when neither circle contains the other, which only happens when the intersection point is on the line segment between the two points.\n",
    "\n",
    "The line segment can be defined as\n",
    "\n",
    "$$\n",
    "r(t) = (1, 0) + t((0, 1) - (1, 0)) \\\\\n",
    "  = (1, 0) + t(-1, 1), t \\geq 0\n",
    "$$\n",
    "\n",
    "and we can sample points on it by varying the value of $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "points = [[1 - t, t] for t in np.linspace(0, 1, 50)]\n",
    "obj_values = [[obj(x) for x in points] for obj in objectives]\n",
    "\n",
    "# decision space\n",
    "plt.figure(figsize=[6, 6])\n",
    "plt.xlim([-1, 2])\n",
    "plt.xlabel('x_1')\n",
    "plt.ylim([-1, 2])\n",
    "plt.ylabel('x_2')\n",
    "\n",
    "plt.plot([x[0] for x in points], [x[1] for x in points], 'o')\n",
    "\n",
    "# objective space\n",
    "plt.figure(figsize=[6, 6])\n",
    "plt.xlim([0, 3])\n",
    "plt.xlabel('f_1')\n",
    "plt.ylim([0, 3])\n",
    "plt.ylabel('f_2')\n",
    "\n",
    "plt.plot(obj_values[0], obj_values[1], 'o')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set is perfectly evenly spaced by construction. If the objective functions were more complicated and this couldn't be done, I would try the epsilon constraint method next, but it's luckily not necessary now."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
