{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIES483 Assignment, Mikael MyyrÃ¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A window is being built and the bottom is a rectangle and the top is a semicircle. If there is 12 m of framing materials what must the dimensions of the window be to make the window area as big as possible?\n",
    "\n",
    "Model the decision problem as an optimization problem and solve it with a method of your choosing. **Analyse the result!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant measurements (circumference and area) of the window can be expressed as functions of width $w$ and height $h$. The circumference is $w + 2h$ for the rectangular part and $\\pi \\frac{w}{2}$ for the circular part, so the total circumference is $C(w, h) = (1 + \\frac{\\pi}{2})w + 2h$. The area is $wh$ for the rectangular part and $\\pi (\\frac{w}{2})^2$ for the circular part, so the total area is $A(w, h) = wh + \\frac{\\pi}{4} w^2$. We're trying to maximize $A$ (and thus minimize $-A$) subject to the linear constraint $C = 12$. Also, $w$ and $h$ must be greater than zero because this is a real window. In formal terms, the problem is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\qquad & -wh - \\frac{\\pi}{4} w^2 \\\\\n",
    "\\text{s.t.} \\qquad & (1 + \\frac{\\pi}{2})w + 2h = 12 \\\\\n",
    "& w, r > 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple case it should be possible to find a solution analytically using the stationarity rule from the KKT conditions. Let's try it. The Lagrangian for this problem is\n",
    "\n",
    "$$\n",
    "L(w, h) = -wh - \\frac{\\pi}{4} w^2 - \\lambda(1 + \\frac{\\pi}{2})w - 2\\lambda h\n",
    "$$\n",
    "\n",
    "and its gradient\n",
    "\n",
    "$$\n",
    "\\nabla L(w, h) = (-h - \\frac{\\pi}{2} w - \\lambda(1 + \\frac{\\pi}{2}), -w - 2\\lambda).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're looking for points where\n",
    "$$\n",
    "\\nabla L(w, h) = \\mathbf{0} \\\\\n",
    "\\begin{cases}\n",
    "-h - \\frac{\\pi}{2}w - \\lambda(1 + \\frac{\\pi}{2}) = 0 \\\\\n",
    "-w - 2\\lambda = 0\n",
    "\\end{cases} \\\\\n",
    "w = -\\frac{\\lambda}{2} \\\\\n",
    "-h + (\\frac{\\pi}{4} - (1 + \\frac{\\pi}{2}))\\lambda = 0 \\iff h = (-\\frac{\\pi}{4} - 1)\\lambda\n",
    "$$\n",
    "\n",
    "Applying the constraint, we find feasible values for $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(1 + \\frac{\\pi}{2})w + 2h &= 12 \\\\\n",
    "(1 + \\frac{\\pi}{2})(-\\frac{\\lambda}{2}) + 2(-\\frac{\\pi}{4} - 1)\\lambda &= 12 \\\\x\n",
    "(-\\frac12 - \\frac{\\pi}{4} - \\frac{\\pi}{2} - 2)\\lambda &= 12 \\\\\n",
    "(-\\frac{3\\pi}{4} - \\frac{5}{2})\\lambda &= 12 \\\\\n",
    "\\lambda &= \\frac{-12}{\\frac{3\\pi}{4} + \\frac{5}{2}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "now, applying $\\lambda$ back to $w$ and $h$, we get\n",
    "\n",
    "$$\n",
    "w = -\\frac{\\lambda}{2} = \\frac{12}{\\frac{3\\pi}{2} + 5} \\approx 1.236 \\\\\n",
    "h = (-\\frac{\\pi}{4} - 1) \\lambda \n",
    "  = \\frac{3\\pi + 12}{\\frac{3\\pi}{4} + \\frac{5}{2}} \\approx 4.412.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: Because we used the KKT conditions to come up with this solution analytically, we already know it's optimal. Just to check that the calculations were correct, you can plug $w$ and $h$ back into the constraint function to ensure it holds. Plugging said values into the objective function and flipping the sign gives the optimal area of the window, which comes out to be approximately $6.653 m^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10-dimensional Robsenbrock function (one of the variants) is defined as\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_{i=1}^{9} 100 (x_{i+1} - x_i^2 )^2 + (1-x_i)^2\n",
    "$$\n",
    "for $x\\in\\mathbb R^{10}$. \n",
    "\n",
    "Compare at least two different optimization method's performance in minimizing this function over $\\mathbb R^{10}$. You can decide the method of comparison as the one that makes most sense to you. **Analyze the results!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's define the problem in Python\n",
    "\n",
    "def obj(x):\n",
    "    assert(len(x) == 10)\n",
    "    ans = 0.0\n",
    "    for i in range(0, 9):\n",
    "        ans += 100*((x[i+1] - x[i]**2)**2) + (1 - x[i])**2\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the function is also needed for a method I want to test. It's a bit complex so I'll write down some steps factorizing the function to make it easier for myself.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(\\mathbf{x}) &= \\sum_{i=1}^9 100(x_{i+1} - x_i^2)^2 + (1 - x_i)^2 \\\\\n",
    "&= \\sum_{i=1}^9 100(x_{i+1}^2 - 2 x_{i+1} x_i^2 + x_i^4) + (1 - 2 x_i + x_i^2) \\\\\n",
    "&= \\sum_{i=1}^9 100x_{i+1}^2 - 200 x_{i+1} x_i^2 + 100 x_i^4 + x_i^2 - 2 x_i + 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The expression for the gradient is so long it's hard to write down, but is fairly straightforward to calculate by iterating the sum and updating corresponding elements in the gradient with the derivatives of the summed expression. The gradient elements obtained from a single iteration of the sum are\n",
    "\n",
    "$$\n",
    "\\nabla_{(i, i+1)} f(\\mathbf{x}) = (-400x_{i+1}x_i + 400x_i^3 + 2x_i - 2, 200x_{i+1} - 200x_{i+1}x_i^2).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def obj_gradient(x):\n",
    "    assert(len(x) == 10)\n",
    "    ans = np.array([0.0] * 10)\n",
    "    for i in range(0, 9):\n",
    "        ans[i] += -400 * x[i+1] * x[i] + 400 * x[i]**3 + 2 * x[i] - 2\n",
    "        ans[i+1] += 200 * x[i+1] - 200 * x[i+1] * x[i]**2\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since scipy.optimize.minimize has multiple methods available and is able to print statistics, it seems like a convenient place to go for this task. Let's pick Nelder-Mead (uses no derivatives) and BFGS (uses first derivatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting point: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Nelder-Mead:\n",
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "Result:\n",
      " final_simplex: (array([[ 0.95819942,  0.92047379,  0.83391187,  0.70387608,  0.52461411,\n",
      "         0.28254586,  0.06659239,  0.08429688, -0.04589073, -0.08591718],\n",
      "       [ 0.95894233,  0.9204341 ,  0.83222897,  0.70545843,  0.5254473 ,\n",
      "         0.2806476 ,  0.06734214,  0.08417792, -0.04925657, -0.08365011],\n",
      "       [ 0.96120856,  0.92602729,  0.84155191,  0.72115311,  0.54131841,\n",
      "         0.29620842,  0.07283483,  0.08501551, -0.05263569, -0.08522277],\n",
      "       [ 0.95987139,  0.92438481,  0.83994796,  0.7175543 ,  0.54184927,\n",
      "         0.3029527 ,  0.07201315,  0.08518736, -0.04873201, -0.08613543],\n",
      "       [ 0.95692642,  0.92085713,  0.83469867,  0.70989731,  0.53229555,\n",
      "         0.28569522,  0.06754217,  0.08580009, -0.04813109, -0.08382746],\n",
      "       [ 0.95642002,  0.91996378,  0.83332828,  0.70289444,  0.51999602,\n",
      "         0.27972976,  0.06984152,  0.08314439, -0.04933047, -0.08598427],\n",
      "       [ 0.95946257,  0.92098857,  0.83384559,  0.70745784,  0.5240412 ,\n",
      "         0.27677058,  0.06734289,  0.08525029, -0.049609  , -0.08399479],\n",
      "       [ 0.95606883,  0.9173665 ,  0.82834773,  0.70270276,  0.52372669,\n",
      "         0.28204846,  0.06847622,  0.08306563, -0.04933393, -0.08358393],\n",
      "       [ 0.9600221 ,  0.92249905,  0.83678707,  0.71316977,  0.53120054,\n",
      "         0.2876331 ,  0.0698738 ,  0.08643524, -0.04921087, -0.08502075],\n",
      "       [ 0.95721847,  0.91773836,  0.82754518,  0.70007521,  0.52133314,\n",
      "         0.27975043,  0.06794376,  0.08456359, -0.04839973, -0.0821389 ],\n",
      "       [ 0.95646391,  0.91850892,  0.8306873 ,  0.70388411,  0.52501665,\n",
      "         0.28522892,  0.06952631,  0.08515477, -0.04838668, -0.08352602]]), array([5.49475967, 5.49903842, 5.49957241, 5.50037358, 5.50073397,\n",
      "       5.50097299, 5.50135384, 5.50273935, 5.50294226, 5.50308465,\n",
      "       5.50318537]))\n",
      "           fun: 5.494759667492491\n",
      "       message: 'Maximum number of function evaluations has been exceeded.'\n",
      "          nfev: 2000\n",
      "           nit: 1458\n",
      "        status: 1\n",
      "       success: False\n",
      "             x: array([ 0.95819942,  0.92047379,  0.83391187,  0.70387608,  0.52461411,\n",
      "        0.28254586,  0.06659239,  0.08429688, -0.04589073, -0.08591718])\n",
      "BFGS:\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 8.620272\n",
      "         Iterations: 7\n",
      "         Function evaluations: 88\n",
      "         Gradient evaluations: 76\n",
      "Result:\n",
      "      fun: 8.620271958944095\n",
      " hess_inv: array([[ 4.68343923e-02, -6.00084517e-02, -1.95639249e-02,\n",
      "        -1.92416200e-02, -2.85789007e-02,  1.11106242e-01,\n",
      "         3.12604776e-04,  8.30543901e-03,  1.55089155e-03,\n",
      "         0.00000000e+00],\n",
      "       [-6.00084517e-02,  8.10590926e-01, -1.76409335e-01,\n",
      "        -1.76697617e-01, -1.75786720e-01, -2.01305431e-01,\n",
      "         8.39784909e-02, -8.89978003e-02, -4.28000632e-02,\n",
      "         0.00000000e+00],\n",
      "       [-1.95639249e-02, -1.76409335e-01,  8.05204051e-01,\n",
      "        -1.95104899e-01, -1.94125274e-01, -2.16470888e-01,\n",
      "        -3.83412433e-02,  1.30474232e-02,  1.61546092e-02,\n",
      "         0.00000000e+00],\n",
      "       [-1.92416200e-02, -1.76697617e-01, -1.95104899e-01,\n",
      "         8.04584365e-01, -1.94430726e-01, -2.16910379e-01,\n",
      "        -3.76266091e-02,  1.38722821e-02,  1.61291670e-02,\n",
      "         0.00000000e+00],\n",
      "       [-2.85789007e-02, -1.75786720e-01, -1.94125274e-01,\n",
      "        -1.94430726e-01,  8.06516289e-01, -2.15147052e-01,\n",
      "        -3.93842162e-02,  1.23057235e-02,  1.79326411e-02,\n",
      "         0.00000000e+00],\n",
      "       [ 1.11106242e-01, -2.01305431e-01, -2.16470888e-01,\n",
      "        -2.16910379e-01, -2.15147052e-01,  7.44716549e-01,\n",
      "         3.16270450e-02,  3.87219544e-02, -8.12758136e-03,\n",
      "         0.00000000e+00],\n",
      "       [ 3.12604776e-04,  8.39784909e-02, -3.83412433e-02,\n",
      "        -3.76266091e-02, -3.93842162e-02,  3.16270450e-02,\n",
      "         1.78127478e-02, -7.76498304e-03, -6.27453110e-03,\n",
      "         0.00000000e+00],\n",
      "       [ 8.30543901e-03, -8.89978003e-02,  1.30474232e-02,\n",
      "         1.38722821e-02,  1.23057235e-02,  3.87219544e-02,\n",
      "        -7.76498304e-03,  1.54743282e-02,  4.09107947e-03,\n",
      "         0.00000000e+00],\n",
      "       [ 1.55089155e-03, -4.28000632e-02,  1.61546092e-02,\n",
      "         1.61291670e-02,  1.79326411e-02, -8.12758136e-03,\n",
      "        -6.27453110e-03,  4.09107947e-03,  8.22306506e-03,\n",
      "         0.00000000e+00],\n",
      "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "         1.00000000e+00]])\n",
      "      jac: array([-0.14001436,  0.30954489,  0.25355373,  0.25417123,  0.26000047,\n",
      "        0.19927732, -0.29928511, -0.34773033,  0.50339937,  0.        ])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 88\n",
      "      nit: 7\n",
      "     njev: 76\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([0.18105005, 0.01209581, 0.01141292, 0.01141635, 0.01143744,\n",
      "       0.011074  , 0.00856132, 0.00838471, 0.01239016, 0.        ])\n",
      "================================================================================\n",
      "Starting point: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Nelder-Mead:\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 386\n",
      "         Function evaluations: 600\n",
      "Result:\n",
      " final_simplex: (array([[1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "        1.        , 1.        , 1.        , 1.        , 1.        ],\n",
      "       [1.00000001, 0.99999999, 0.99999999, 0.99999999, 0.99999999,\n",
      "        1.        , 0.99999999, 1.00000001, 1.00000003, 1.00000007],\n",
      "       [1.        , 1.        , 1.00000001, 0.99999999, 0.99999998,\n",
      "        0.99999996, 0.99999992, 0.99999987, 0.99999974, 0.99999947],\n",
      "       [1.        , 1.        , 1.00000001, 1.00000001, 1.        ,\n",
      "        1.00000001, 1.        , 0.99999999, 0.99999994, 0.99999988],\n",
      "       [1.        , 1.        , 1.00000002, 1.        , 0.99999998,\n",
      "        0.99999999, 0.99999996, 0.99999994, 0.9999999 , 0.99999981],\n",
      "       [1.00000002, 0.99999999, 1.        , 1.        , 1.        ,\n",
      "        0.99999998, 0.99999997, 0.99999996, 0.9999999 , 0.99999984],\n",
      "       [1.        , 0.99999999, 1.00000002, 1.00000001, 1.00000002,\n",
      "        1.00000002, 1.00000003, 1.0000001 , 1.00000016, 1.00000032],\n",
      "       [0.99999999, 1.00000001, 1.00000001, 1.00000001, 1.00000001,\n",
      "        1.        , 0.99999999, 1.00000003, 1.00000004, 1.0000001 ],\n",
      "       [1.00000001, 1.00000001, 1.00000001, 0.99999998, 0.99999996,\n",
      "        0.99999996, 0.99999994, 0.99999991, 0.99999983, 0.99999967],\n",
      "       [1.        , 1.00000001, 1.00000003, 1.        , 1.00000002,\n",
      "        1.00000002, 1.00000001, 1.00000004, 1.00000008, 1.0000002 ],\n",
      "       [0.99999997, 0.99999998, 0.99999999, 0.99999998, 0.99999999,\n",
      "        0.99999998, 0.99999996, 0.99999992, 0.99999984, 0.99999972]]), array([0.00000000e+00, 2.48122931e-13, 2.63639554e-13, 2.99659915e-13,\n",
      "       3.14808643e-13, 4.70344137e-13, 5.24313850e-13, 5.69781819e-13,\n",
      "       5.82396162e-13, 6.60679608e-13, 6.81552884e-13]))\n",
      "           fun: 0.0\n",
      "       message: 'Optimization terminated successfully.'\n",
      "          nfev: 600\n",
      "           nit: 386\n",
      "        status: 0\n",
      "       success: True\n",
      "             x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "BFGS:\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 0\n",
      "         Function evaluations: 1\n",
      "         Gradient evaluations: 1\n",
      "Result:\n",
      "      fun: 0.0\n",
      " hess_inv: array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
      "      jac: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 1\n",
      "      nit: 0\n",
      "     njev: 1\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "================================================================================\n",
      "Starting point: [2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "Nelder-Mead:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "Result:\n",
      " final_simplex: (array([[0.99594434, 1.00061315, 0.99899839, 0.99894412, 1.00421332,\n",
      "        1.01327232, 1.01922871, 1.03531195, 1.0651717 , 1.13458539],\n",
      "       [0.99579141, 1.00105939, 0.99869556, 0.998967  , 1.0043477 ,\n",
      "        1.01188324, 1.01592046, 1.02636383, 1.0466271 , 1.09758113],\n",
      "       [0.99605162, 1.00083998, 0.99939916, 0.99842367, 1.00409885,\n",
      "        1.0119609 , 1.01652826, 1.02638101, 1.04683471, 1.09866375],\n",
      "       [0.99567335, 1.00064177, 0.99960383, 1.00014888, 1.00429823,\n",
      "        1.01392193, 1.01794475, 1.03268623, 1.05995148, 1.12322089],\n",
      "       [0.99696348, 1.00287916, 0.99978023, 0.99895393, 1.00450958,\n",
      "        1.01203808, 1.01501725, 1.02643147, 1.04707779, 1.09823168],\n",
      "       [0.99626718, 1.00122233, 0.99862005, 0.99973699, 1.0030887 ,\n",
      "        1.0136106 , 1.02077926, 1.03623261, 1.0670106 , 1.1409193 ],\n",
      "       [0.99588546, 1.00120275, 0.99969038, 0.99877669, 1.00350816,\n",
      "        1.01356892, 1.02016055, 1.03525761, 1.06553596, 1.13659802],\n",
      "       [0.99688491, 1.0009279 , 0.99870239, 0.99868266, 1.00506105,\n",
      "        1.0145002 , 1.01931836, 1.03570257, 1.06586327, 1.136172  ],\n",
      "       [0.99737973, 1.00171612, 0.99956403, 0.99800886, 1.00425776,\n",
      "        1.01309708, 1.01793983, 1.0311909 , 1.05572826, 1.11701668],\n",
      "       [0.99534629, 0.99860967, 0.99948626, 0.99944796, 1.0048926 ,\n",
      "        1.01558462, 1.02305132, 1.04202053, 1.079289  , 1.16549611],\n",
      "       [0.99643822, 1.00128601, 0.99894513, 0.99980725, 1.0050296 ,\n",
      "        1.01453359, 1.02090193, 1.03910217, 1.07462104, 1.16126319]]), array([0.03191272, 0.03353316, 0.03382018, 0.03425723, 0.03468514,\n",
      "       0.0357417 , 0.03576506, 0.03584489, 0.03595241, 0.03598218,\n",
      "       0.03621141]))\n",
      "           fun: 0.03191272070758676\n",
      "       message: 'Maximum number of function evaluations has been exceeded.'\n",
      "          nfev: 2001\n",
      "           nit: 1433\n",
      "        status: 1\n",
      "       success: False\n",
      "             x: array([0.99594434, 1.00061315, 0.99899839, 0.99894412, 1.00421332,\n",
      "       1.01327232, 1.01922871, 1.03531195, 1.0651717 , 1.13458539])\n",
      "BFGS:\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 1650.521452\n",
      "         Iterations: 1\n",
      "         Function evaluations: 42\n",
      "         Gradient evaluations: 30\n",
      "Result:\n",
      "      fun: 1650.5214524211724\n",
      " hess_inv: array([[ 0.22403795,  0.24325457, -0.05962776, -0.05962776, -0.05962776,\n",
      "        -0.05962776, -0.05962776, -0.05962776, -0.31876285, -0.04017804],\n",
      "       [ 0.24325457,  1.1709444 ,  0.09494023,  0.09494023,  0.09494023,\n",
      "         0.09494023,  0.09494023,  0.09494023,  0.02991382, -0.33815077],\n",
      "       [-0.05962776,  0.09494023,  1.01893605,  0.01893605,  0.01893605,\n",
      "         0.01893605,  0.01893605,  0.01893605, -0.04609036, -0.11127263],\n",
      "       [-0.05962776,  0.09494023,  0.01893605,  1.01893605,  0.01893605,\n",
      "         0.01893605,  0.01893605,  0.01893605, -0.04609036, -0.11127263],\n",
      "       [-0.05962776,  0.09494023,  0.01893605,  0.01893605,  1.01893605,\n",
      "         0.01893605,  0.01893605,  0.01893605, -0.04609036, -0.11127263],\n",
      "       [-0.05962776,  0.09494023,  0.01893605,  0.01893605,  0.01893605,\n",
      "         1.01893605,  0.01893605,  0.01893605, -0.04609036, -0.11127263],\n",
      "       [-0.05962776,  0.09494023,  0.01893605,  0.01893605,  0.01893605,\n",
      "         0.01893605,  1.01893605,  0.01893605, -0.04609036, -0.11127263],\n",
      "       [-0.05962776,  0.09494023,  0.01893605,  0.01893605,  0.01893605,\n",
      "         0.01893605,  0.01893605,  1.01893605, -0.04609036, -0.11127263],\n",
      "       [-0.31876285,  0.02991382, -0.04609036, -0.04609036, -0.04609036,\n",
      "        -0.04609036, -0.04609036, -0.04609036,  0.88888323,  0.08283606],\n",
      "       [-0.04017804, -0.33815077, -0.11127263, -0.11127263, -0.11127263,\n",
      "        -0.11127263, -0.11127263, -0.11127263,  0.08283606,  1.49558132]])\n",
      "      jac: array([  -72.60864294,   848.3269479 ,   249.06029234,   249.06029234,\n",
      "         249.06029234,   249.06029234,   249.06029234,   249.06029234,\n",
      "        -263.6504525 , -1175.12562657])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 42\n",
      "      nit: 1\n",
      "     njev: 30\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([1.29712636, 1.82362347, 1.82362347, 1.82362347, 1.82362347,\n",
      "       1.82362347, 1.82362347, 1.82362347, 1.82362347, 2.52649711])\n",
      "================================================================================\n",
      "Starting point: [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      "Nelder-Mead:\n",
      "Warning: Maximum number of function evaluations has been exceeded.\n",
      "Result:\n",
      " final_simplex: (array([[ 4.89014900e-01,  2.51113998e-01,  3.30060714e-02,\n",
      "         4.77126703e-02,  4.69882172e-02,  2.25431617e-02,\n",
      "        -2.02245929e-03, -4.64418087e+00,  2.20970204e+01,\n",
      "         4.88316448e+02],\n",
      "       [ 4.76428987e-01,  2.41688041e-01,  4.14773276e-02,\n",
      "         6.65893248e-02,  4.35233314e-02,  1.29780149e-02,\n",
      "        -1.08615392e-03, -4.64523022e+00,  2.20970615e+01,\n",
      "         4.88312997e+02],\n",
      "       [ 4.57673160e-01,  2.24195633e-01,  4.65206769e-02,\n",
      "         5.07830256e-02,  4.06301447e-02,  2.18378464e-02,\n",
      "        -7.72015423e-03, -4.64570356e+00,  2.20974393e+01,\n",
      "         4.88336416e+02],\n",
      "       [ 4.66747750e-01,  2.24963635e-01,  3.61565210e-02,\n",
      "         4.52821075e-02,  4.46821312e-02,  1.30364672e-02,\n",
      "        -9.87687613e-04, -4.64342812e+00,  2.20975162e+01,\n",
      "         4.88344498e+02],\n",
      "       [ 4.73672570e-01,  2.36159061e-01,  4.01688572e-02,\n",
      "         6.02324083e-02,  3.15137868e-02,  4.46978637e-02,\n",
      "         2.29996351e-03, -4.64428009e+00,  2.20968546e+01,\n",
      "         4.88316041e+02],\n",
      "       [ 4.68018851e-01,  2.23783092e-01,  3.85636586e-02,\n",
      "         6.84191281e-02,  2.78820161e-02,  1.81863072e-02,\n",
      "        -2.24256965e-03, -4.64456749e+00,  2.20973228e+01,\n",
      "         4.88334122e+02],\n",
      "       [ 4.95342574e-01,  2.27705169e-01,  3.48415301e-02,\n",
      "         6.72264480e-02,  3.67547061e-02,  1.43613737e-02,\n",
      "         8.79056200e-04, -4.64421857e+00,  2.20973354e+01,\n",
      "         4.88331525e+02],\n",
      "       [ 4.82226953e-01,  2.38175149e-01,  2.95241758e-02,\n",
      "         6.03948472e-02,  3.30727368e-02,  2.46010605e-02,\n",
      "        -6.31254534e-03, -4.64574575e+00,  2.20971353e+01,\n",
      "         4.88329974e+02],\n",
      "       [ 4.46388581e-01,  2.11147034e-01,  2.68798043e-02,\n",
      "         3.90517546e-02,  4.52282604e-02,  3.60010470e-02,\n",
      "         3.35795820e-03, -4.64582363e+00,  2.20981578e+01,\n",
      "         4.88362724e+02],\n",
      "       [ 4.70458197e-01,  2.35216093e-01,  3.70852550e-02,\n",
      "         6.04942105e-02,  5.58565378e-02,  1.61017530e-02,\n",
      "        -5.22748285e-03, -4.64502490e+00,  2.20972433e+01,\n",
      "         4.88324685e+02],\n",
      "       [ 4.48859451e-01,  2.02848925e-01,  4.24945665e-02,\n",
      "         6.64761175e-02,  3.57475298e-02,  2.64484265e-02,\n",
      "        -5.85963093e-03, -4.64543982e+00,  2.20982847e+01,\n",
      "         4.88352113e+02]]), array([2667.96965569, 2667.97238892, 2668.00466712, 2668.05990427,\n",
      "       2668.06581727, 2668.07886911, 2668.0920843 , 2668.10152005,\n",
      "       2668.10384297, 2668.12664838, 2668.1353697 ]))\n",
      "           fun: 2667.96965568853\n",
      "       message: 'Maximum number of function evaluations has been exceeded.'\n",
      "          nfev: 2001\n",
      "           nit: 1439\n",
      "        status: 1\n",
      "       success: False\n",
      "             x: array([ 4.89014900e-01,  2.51113998e-01,  3.30060714e-02,  4.77126703e-02,\n",
      "        4.69882172e-02,  2.25431617e-02, -2.02245929e-03, -4.64418087e+00,\n",
      "        2.20970204e+01,  4.88316448e+02])\n",
      "BFGS:\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 50360024165.476677\n",
      "         Iterations: 2\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 101\n",
      "Result:\n",
      "      fun: 50360024165.47668\n",
      " hess_inv: array([[ 0.09449433,  0.23561286, -0.10428015, -0.05330233, -0.05330233,\n",
      "        -0.05330233, -0.05330233, -0.05328614, -0.05611119,  0.05935912],\n",
      "       [ 0.23561286,  0.62189872, -0.11136559, -0.16648375, -0.16648375,\n",
      "        -0.16648375, -0.16648375, -0.16650126, -0.16563561,  0.06042067],\n",
      "       [-0.10428015, -0.11136559,  0.93840439, -0.08361933, -0.08361933,\n",
      "        -0.08361933, -0.08361933, -0.08362633, -0.08451451, -0.01707419],\n",
      "       [-0.05330233, -0.16648375, -0.08361933,  0.89435694, -0.10564306,\n",
      "        -0.10564306, -0.10564306, -0.10565005, -0.10581933,  0.03956576],\n",
      "       [-0.05330233, -0.16648375, -0.08361933, -0.10564306,  0.89435694,\n",
      "        -0.10564306, -0.10564306, -0.10565005, -0.10581933,  0.03956576],\n",
      "       [-0.05330233, -0.16648375, -0.08361933, -0.10564306, -0.10564306,\n",
      "         0.89435694, -0.10564306, -0.10565005, -0.10581933,  0.03956576],\n",
      "       [-0.05330233, -0.16648375, -0.08361933, -0.10564306, -0.10564306,\n",
      "        -0.10564306,  0.89435694, -0.10565005, -0.10581933,  0.03956576],\n",
      "       [-0.05328614, -0.16650126, -0.08362633, -0.10565005, -0.10565005,\n",
      "        -0.10565005, -0.10565005,  0.89434295, -0.1058261 ,  0.03958376],\n",
      "       [-0.05611119, -0.16563561, -0.08451451, -0.10581933, -0.10581933,\n",
      "        -0.10581933, -0.10581933, -0.1058261 ,  0.89403103,  0.04296499],\n",
      "       [ 0.05935912,  0.06042067, -0.01707419,  0.03956576,  0.03956576,\n",
      "         0.03956576,  0.03956576,  0.03958376,  0.04296499,  1.27508516]])\n",
      "      jac: array([ 7.27754679e+08, -9.77734528e+07,  1.45749758e+08,  9.77123080e+07,\n",
      "        9.77123080e+07,  9.77123080e+07,  9.77123080e+07,  9.76970457e+07,\n",
      "        9.86916094e+07, -1.83641858e+08])\n",
      "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "     nfev: 113\n",
      "      nit: 2\n",
      "     njev: 101\n",
      "   status: 2\n",
      "  success: False\n",
      "        x: array([122.23567915,  57.31384693,  79.42816149,  79.42816149,\n",
      "        79.42816149,  79.42816149,  79.42816149,  79.42816149,\n",
      "        79.90854391, 143.82131939])\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "first_guesses = [\n",
    "    # try a few starting points varying distances away\n",
    "    np.array([0.0] * 10),\n",
    "    np.array([1.0] * 10),\n",
    "    np.array([2.0] * 10),\n",
    "    np.array([100.0] * 10),\n",
    "]\n",
    "\n",
    "for x0 in first_guesses:\n",
    "    print('=' * 80)\n",
    "    print(f'Starting point: {x0}')\n",
    "    \n",
    "    print('Nelder-Mead:')\n",
    "    nm_result = minimize(\n",
    "        obj,\n",
    "        x0,\n",
    "        method='Nelder-Mead',\n",
    "        tol=1e-6,\n",
    "        options={'disp': True},\n",
    "    )\n",
    "    print('Result:')\n",
    "    print(nm_result)\n",
    "    \n",
    "    print('BFGS:')\n",
    "    bfgs_result = minimize(\n",
    "        obj,\n",
    "        x0,\n",
    "        method='BFGS',\n",
    "        jac=obj_gradient,\n",
    "        tol=1e-6,\n",
    "        options={'disp': True},\n",
    "    )\n",
    "    print('Result:')\n",
    "    print(bfgs_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods produce very different results depending on the chosen starting point. I think this is because both methods run out of allowed iterations before reaching convergence, and the function has a very flat minimal region (in the 2D case it's shaped like a parabola; in this case it's impossible to visualize but I assume it's something like a 10D paraboloid). Thus all points on this paraboloid-ish surface have objective function values very close to the minimum.\n",
    "\n",
    "Nelder-Mead seems to take thousands of iterations if we start from anywhere that isn't already in the minimum region. BFGS, on the other hand, takes an order of magnitude fewer function evaluations, which is expected as it uses gradient information to find good search directions. However, BFGS doesn't appear to find better results. On the contrary, in the last example of starting at (100, ..., 100), BFGS comes up with a much worse solution than Nelder-Mead. This seems strange, but it does look like the gradient at the point it finds is close to zero. This may be because the shape of the function is very flat, but I think I've probably made a mistake in my gradient formulation. I've looked over the calculations many times and I can't find a mistake, but this result seems very strange otherwise.\n",
    "\n",
    "Note: I calculated the gradient by hand because I don't have easy access to the `ad` library on my OS. Using that library would be an easy way to check if the results are the same with a gradient that's known to be correct.\n",
    "\n",
    "(1, ..., 1) looks like it's an optimal solution as both methods return it if we start there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to solve a black-box optimization problem where the objective and constraint function values can be obtained by calling an executable. The executable <i>prob3</i> will be available at the course website http://users.jyu.fi/~jhaka/opt/ along with instructions on how to use it in the <i>README</i> file. \n",
    "\n",
    "The format of the problem is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\ &f(x)\\\\\n",
    "\\text{s.t. }&h_1(x) = 0\\\\\n",
    "        &h_2(x) = 0\\\\\n",
    "        &g_1(x) \\geq 0\\\\\n",
    "        &g_2(x) \\geq 0\\\\\n",
    "        &g_3(x) \\geq 0\\\\\n",
    "        &g_4(x) \\geq 0\\\\\n",
    "        &x\\in \\mathbb R^4.        \n",
    "\\end{align}\n",
    "$$\n",
    "Solve the optimization problem by using the tools and optimization method of your choosing. **Analyse the results!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study biobjective optimization problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\ &(\\|x-(1,0)\\|,\\|x-(0,1)\\|)\\\\\n",
    "\\text{s.t. }&x\\in \\mathbb R^2.\n",
    "\\end{align}\n",
    "$$\n",
    "Try to generate an evenly spread representation of the Pareto front. Plot the results in both the decision and objective spaces. **Analyze the results!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
