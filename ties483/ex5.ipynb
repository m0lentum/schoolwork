{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIES483 ex5, Mikael Myyr√§\n",
    "Note: Switching from Haskell to Python as implementing all the linear algebra by hand became too laborious, and I didn't want to require libraries because you can't submit the whole project directory on Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "For exercises 1-2, we study optimization problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\qquad & x_1^2+x_2^2 + x_3^3+(1-x_4)^2\\\\\n",
    "\\text{s.t.}\\qquad &x_1^2+x_2^2-1=0\\\\\n",
    "    &x_1^2+x_3^2-1=0\\\\\n",
    "    &x\\in\\mathbb R^4\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the problem\n",
    "import numpy as np\n",
    "\n",
    "def objective_fn(x):\n",
    "    return x[0]**2 + x[1]**2 + x[2]**3 + (1 - x[3])**2\n",
    "\n",
    "eq_constraints = [\n",
    "    lambda x: x[0]**2 + x[1]**2 - 1,\n",
    "    lambda x: x[0]**2 + x[2]**2 - 1\n",
    "]\n",
    "\n",
    "# ad is not available on nixpkgs (my Linux distro's package repository)\n",
    "# and I don't want to use pip, so I calculate the derivatives by hand\n",
    "\n",
    "def obj_gradient(x):\n",
    "    return np.matrix([2*x[0], 2*x[1], 3*(x[2]**2), 2*x[3]])\n",
    "\n",
    "eq_constr_gradients = [\n",
    "    lambda x: np.matrix([2*x[0], 2*x[1], 0, 0]),\n",
    "    lambda x: np.matrix([2*x[0], 0, 2*x[2], 0])\n",
    "]\n",
    "\n",
    "def obj_hessian(x):\n",
    "    return np.matrix([\n",
    "        [2, 0, 0, 0],\n",
    "        [0, 2, 0, 0],\n",
    "        [0, 0, 6*x[2], 0],\n",
    "        [0, 0, 0, 2]\n",
    "    ])\n",
    "\n",
    "eq_constr_hessians = [\n",
    "    lambda x: np.matrix([\n",
    "        [2, 0, 0, 0],\n",
    "        [0, 2, 0, 0],\n",
    "        [0, 0, 0, 0],\n",
    "        [0, 0, 0, 0]\n",
    "    ]),\n",
    "    lambda x: np.matrix([\n",
    "        [2, 0, 0, 0],\n",
    "        [0, 0, 0, 0],\n",
    "        [0, 0, 2, 0],\n",
    "        [0, 0, 0, 0]\n",
    "    ]),\n",
    "]\n",
    "\n",
    "def lagrangian(x, l):\n",
    "    return objective_fn(x) \\\n",
    "        + sum([l[i] * eq_constraints[i](x) \\\n",
    "              for i in range(len(eq_constraints))])\n",
    "\n",
    "def lagr_gradient(x, l):\n",
    "    return obj_gradient(x) \\\n",
    "        + sum([l[i] * eq_constr_gradients[i](x) \\\n",
    "              for i in range(len(eq_constraints))])\n",
    "\n",
    "def lagr_hessian(x, l):\n",
    "    return obj_hessian(x) \\\n",
    "        + sum([l[i] * eq_constr_hessians[i](x) \\\n",
    "              for i in range(len(eq_constraints))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **(2 points)** Use the SQP method to solve the above problem. **Analyze carefully the result you got!** How does SQP work for this problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeatedly solving\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "\\nabla^2_{xx}L(x^k,\\lambda^k)&\\nabla_x h(x^k)\\\\\n",
    "\\nabla_x h(x^k)^T & 0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\\begin{array}{c}p^T\\\\v^T\\end{array}\\right] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "-\\nabla_x L(x^k,\\lambda^k)\\\\\n",
    "-h(x^k)^T\n",
    "\\end{array}\n",
    "\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at (x: [1, 5, -3, 2], l: [5, 5]) (took 7 steps):\n",
      "\tx: [ 6.0506e-08  1.0000e+00 -1.0000e+00  0.0000e+00]\n",
      "\tf(x):     1.0000\n",
      "\th(x): [1.0027e-07 9.7151e-08]\n",
      "\n",
      "Starting at (x: [13, 18, 0, -1000], l: [10, -15]) (took 11 steps):\n",
      "\tx: [1.     0.0176 0.     0.    ]\n",
      "\tf(x):     2.0003\n",
      "\th(x): [0.0003 0.    ]\n",
      "\n",
      "Starting at (x: [-2, -2, -2, -2], l: [2, 2]) (took 7 steps):\n",
      "\tx: [-7.5208e-12 -1.0000e+00 -1.0000e+00  0.0000e+00]\n",
      "\tf(x):     1.0000\n",
      "\th(x): [1.3554e-11 1.3554e-11]\n",
      "\n",
      "Starting at (x: [1, 1, 1, 1], l: [1, 1]) (took 8 steps):\n",
      "\tx: [1.     0.0123 0.0123 0.    ]\n",
      "\tf(x):     2.0002\n",
      "\th(x): [0.0002 0.0002]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# note: hardcoded objective function and constraints\n",
    "# because this isn't needed for general-purpose use\n",
    "# and piping in all the derivatives through parameters would be annoying\n",
    "\n",
    "def solve_quadratic(x, l):\n",
    "    \"\"\"Solve the linear system defined in the above cell.\"\"\"    \n",
    "    # coefficient matrix\n",
    "    constr_gradients = np.stack([cg(x) for cg in eq_constr_gradients])\n",
    "    c_len = len(constr_gradients)\n",
    "    coefs = np.concatenate((\n",
    "        np.concatenate(\n",
    "            (lagr_hessian(x, l), constr_gradients.transpose()),\n",
    "            axis=1,\n",
    "        ),\n",
    "        np.concatenate(\n",
    "            (constr_gradients, np.zeros((c_len, c_len))),\n",
    "            axis=1,\n",
    "        )), \n",
    "        axis=0,\n",
    "    )\n",
    "    \n",
    "    # right-hand side\n",
    "    constr_values = np.matrix([c(x) for c in eq_constraints])\n",
    "    rhs = -1 * np.concatenate(\n",
    "        (lagr_gradient(x, l), constr_values),\n",
    "        axis=1,\n",
    "    ).transpose()\n",
    "    \n",
    "    solution = np.linalg.solve(coefs, rhs)\n",
    "    return np.array(solution[:len(x)].transpose())[0], \\\n",
    "        np.array(solution[len(x):].transpose())[0]\n",
    "    \n",
    "def sequential_quadratic(start_x, start_l, precision, error_tolerance):\n",
    "    \"\"\"Solve an optimization problem using the SQP method.\"\"\"\n",
    "    x = start_x\n",
    "    l = start_l\n",
    "    # record steps for playback later\n",
    "    steps = [[x, l]]\n",
    "    while True:\n",
    "        x_step, l_step = solve_quadratic(x, l)\n",
    "        x_new = x + x_step\n",
    "        l_new = l + l_step\n",
    "        \n",
    "        steps.append([x_new, l_new])\n",
    "        \n",
    "        # stop if the difference in objective function values\n",
    "        # is small enough and there's not too much constraint error\n",
    "        if abs(objective_fn(x_new) - objective_fn(x)) <= precision \\\n",
    "            and all([abs(c(x_new)) <= error_tolerance for c in eq_constraints]):\n",
    "            return x_new, steps\n",
    "        \n",
    "        # otherwise, loop\n",
    "        x = x_new\n",
    "        l = l_new\n",
    "        \n",
    "        \n",
    "# test the above with some starting points\n",
    "starts = [\n",
    "    # this gives an error because it results in a non-invertible matrix\n",
    "    # due to constraint gradients being zero\n",
    "    # [[0, 0, 0, 0], [0, 0]],\n",
    "    [[1, 5, -3, 2], [5, 5]],\n",
    "    [[13, 18, 0, -1000], [10, -15]],\n",
    "    [[-2, -2, -2, -2], [2, 2]],\n",
    "    [[1, 1, 1, 1], [1, 1]],\n",
    "]\n",
    "precision = 0.001\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "for [x, l] in starts:\n",
    "    solution, steps = sequential_quadratic(x, l, precision, precision)\n",
    "    print(f\"Starting at (x: {x}, l: {l}) (took {len(steps)} steps):\")\n",
    "    print(f\"\\tx: {solution}\")\n",
    "    print(f\"\\tf(x): {objective_fn(solution):10.4f}\")\n",
    "    print(f\"\\th(x): {np.array([c(solution) for c in eq_constraints])}\")\n",
    "    # print(\"steps:\")\n",
    "    # for [x, l] in steps:\n",
    "    #    print(f\"x: {x}, l: {l}\")\n",
    "    #    print(f\"\\tf(x): {objective_fn(x):10.4f}\")\n",
    "    #    print(f\"\\tconstraint values: {np.array([c(x) for c in eq_constraints])}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "From every starting point I've picked at random for testing, it looks to take roughly the same number of steps to find a solution; even the starting point that's 1000 units away on the x4 axis is solved in 11 steps.\n",
    "This makes sense as the lagrangian is quadratic and thus the second order approximation used is accurate from long distance.\n",
    "\n",
    "There are two different local solutions that the algorithm seems to find depending on starting point. I suppose this is because there are two different surfaces where the constraint functions intersect and the algorithm follows whichever one it finds first. One of the solutions has a better objective function value, so only one of them is global, which shows that the algorithm is not guaranteed to find the globally best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(2 points)** Use the augmented Lagrangian method to solve the above problem. **Analyze carefully the result you got!** How does the method work for this problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmented Lagrangian is\n",
    "$$\n",
    "L_c(x,\\lambda) = f(x)+\\lambda h(x)+\\frac12c\\|h(x)\\|^2 = L(x,\\lambda) + c\\|h(x)\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_lagrangian(x, l, penalty):\n",
    "    return lagrangian(x, l) + 0.5 * penalty * sum([c(x)**2 for c in eq_constraints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def aug_lagrangian_solve(start_x, start_l, start_penalty, precision, error_tolerance):\n",
    "    x = start_x\n",
    "    l = np.array(start_l, dtype=float)\n",
    "    pen = start_penalty\n",
    "    # record steps for playback later\n",
    "    steps = [[x, l, pen]]\n",
    "    while True:\n",
    "        x_new = minimize(lambda x: aug_lagrangian(x, l, pen), x).x\n",
    "        # stop if the difference in objective function values\n",
    "        # is small enough and there's not too much constraint error\n",
    "        if abs(objective_fn(x_new) - objective_fn(x)) <= precision \\\n",
    "            and all([abs(c(x_new)) <= error_tolerance for c in eq_constraints]):\n",
    "            return x_new, steps\n",
    "        # otherwise, loop\n",
    "        l += pen * np.array([c(x_new) for c in eq_constraints])\n",
    "        pen *= 2\n",
    "        x = x_new\n",
    "        steps.append([x, l, pen])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at (x: [1, 5, -3, 2], l: [5, 5]) (took 10 steps):\n",
      "\tx: [-2.3367e-07 -1.0000e+00 -1.0000e+00  1.0000e+00]\n",
      "\tf(x):    -0.0000\n",
      "\th(x): [7.4554e-09 6.2676e-08]\n",
      "\n",
      "Starting at (x: [13, 18, 0, -1000], l: [10, -15]) (took 9 steps):\n",
      "\tx: [9.9996e-01 1.3740e-08 1.2580e-02 1.0000e+00]\n",
      "\tf(x):     0.9999\n",
      "\th(x): [-7.9135e-05  7.9122e-05]\n",
      "\n",
      "Starting at (x: [-2, -2, -2, -2], l: [2, 2]) (took 7 steps):\n",
      "\tx: [ 2.2268e-07  1.0000e+00 -1.0000e+00  1.0000e+00]\n",
      "\tf(x):    -0.0000\n",
      "\th(x): [-3.6717e-08  9.2714e-07]\n",
      "\n",
      "Starting at (x: [1, 1, 1, 1], l: [1, 1]) (took 8 steps):\n",
      "\tx: [ 9.9991e-01 -2.5709e-08  1.9189e-02  1.0000e+00]\n",
      "\tf(x):     0.9998\n",
      "\th(x): [-0.0002  0.0002]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using same starting points and precision defined for SQP method\n",
    "start_penalty = 1.0\n",
    "for [x, l] in starts:\n",
    "    solution, steps = aug_lagrangian_solve(x, l, start_penalty, precision, precision)\n",
    "    print(f\"Starting at (x: {x}, l: {l}) (took {len(steps)} steps):\")\n",
    "    print(f\"\\tx: {solution}\")\n",
    "    print(f\"\\tf(x): {objective_fn(solution):10.4f}\")\n",
    "    print(f\"\\th(x): {np.array([c(solution) for c in eq_constraints])}\")\n",
    "    # print(\"steps:\")\n",
    "    # for [x, l, pen] in steps:\n",
    "    #     print(f\"x: {x}, l: {l}, penalty: {pen}\")\n",
    "    #     print(f\"\\tf(x): {objective_fn(x):10.4f}\")\n",
    "    #     print(f\"\\th(x): {np.array([c(x) for c in eq_constraints])}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "\n",
    "This one takes roughly the same number of steps as SQP. An individual step is probably also roughly equal in time cost since this solves an optimization problem per step and SQP inverts a matrix, both of which are somewhat expensive operations.\n",
    "\n",
    "From some starting points this finds another optimum that SQP missed, however, I'm pretty sure it was only missed before because I'm not using very many starting points. So both algorithms give only locally optimal results, but augmented Lagrangian found some better points by luck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(2 points)** Solve the problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min   \\  & x_1^2 + x_2^2\\\\\n",
    "\\text{s.t. } & x_1 + x_2 \\geq 1.\n",
    "\\end{align}\n",
    "$$ \n",
    "by using just the optimality conditions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general form of the problem is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\quad f(x)\\\\\n",
    "\\text{s.t. } \\quad g(x) \\gt 0\n",
    "\\end{align}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f(x) = x_1^2 + x_2^2\\\\\n",
    "g(x) = x_1 + x_2 - 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lagrangian of this is\n",
    "$$\n",
    "L(x,\\mu) = f(x) - \\mu g(x) = x_1^2 + x_2^2 - \\mu (x_1 + x_2 - 1)\n",
    "$$\n",
    "and its gradient with respect to $x$\n",
    "$$\n",
    "\\nabla_x L(x,\\mu) = (2x_1 - \\mu, 2x_2 - \\mu)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stationarity condition states that\n",
    "$$\n",
    "\\nabla_x L(x^*,\\mu^*) = \\mathbf{0}\n",
    "$$\n",
    "Therefore\n",
    "$$\n",
    "(2x_1^* - \\mu^*, 2x_2^* - \\mu^*) = (0, 0)\\\\\n",
    "\\begin{cases}\n",
    "2x_1^* - \\mu^* = 0\\\\\n",
    "2x_2^* - \\mu^* = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "Solving this gives the line\n",
    "$$\n",
    "x_1^* = x_2^* = \\frac{\\mu^*}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the dual feasibility condition states that\n",
    "$$\n",
    "\\mu^* \\geq 0\n",
    "$$\n",
    "Combined with the line we got from stationarity we get\n",
    "$$\n",
    "x_1^*, x_2^* \\geq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, primal feasibility states that\n",
    "$$\n",
    "g(x*) \\geq 0\\\\\n",
    "x_1^* + x_2^* - 1 \\geq 0\n",
    "$$\n",
    "applying $x_1^* = x_2^*$ from stationarity\n",
    "$$\n",
    "2x_1^* - 1 \\geq 0\\\\\n",
    "x_1^*, x_2^* \\geq \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $f(x)$ increases when $x_1$ and $x_2$ increase, the optimal point is the closest point to the origin that satisfies the above conditions, i.e.\n",
    "$$\n",
    "x^* = (\\frac{1}{2}, \\frac{1}{2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **(2 points)** Consider a problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min   \\  & f(x)\\\\\n",
    "\\text{s.t. } & h_k(x)=0, \\text{ for all } k=1,\\dots,K,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where all the functions are twice differentiable. Show, that the *gradient of the augmented Lagrangian function* is zero in the minimizer $x^*$ of the above problem. In other words, show that $\\nabla_xL_c(x^*,\\lambda^*)=0$, where $\\lambda^*\\in R^n$ is the corresponding optimal Lagrange multiplier vector."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
